{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T12:06:38.683264Z",
     "start_time": "2019-05-11T12:06:38.675242Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:12:18.288471Z",
     "start_time": "2019-05-11T11:12:18.285463Z"
    }
   },
   "outputs": [],
   "source": [
    "# get current directory\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:56:41.455736Z",
     "start_time": "2019-05-11T11:56:41.451726Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_files(path: str, label: str):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(path+os.sep+label+os.sep+'*.txt')\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label))\n",
    "        f.close()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:56:41.601583Z",
     "start_time": "2019-05-11T11:56:41.596569Z"
    }
   },
   "outputs": [],
   "source": [
    "def regex(string: str, model: str):\n",
    "    \"\"\"\n",
    "    Function that computes regular expressions.\n",
    "    \"\"\"\n",
    "    string = re.sub(\"[0-9]\", \"0\", string) # each digit will be represented as a 0\n",
    "    string = re.sub(r'( \\n| \\t)+', '', string)\n",
    "    #text = re.sub(\"[0-9]+(([.,^])[0-9]+)?\", \"#\", text)\n",
    "    string = re.sub(\"https:\\\\\\+([a-zA-Z0-9.]+)?\", \"@\", string)\n",
    "    \n",
    "    if model == 'word':\n",
    "        # if model is a word n-gram model, remove all punctuation\n",
    "        string = ''.join([char for char in string if char.isalnum()])\n",
    "        \n",
    "    if model == 'char-dist':\n",
    "        string = re.sub(\"[a-zA-Z]+\", \"*\", string)\n",
    "        # string = ''.join(['*' if char.isalpha() else char for char in string])\n",
    "        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:56:41.740281Z",
     "start_time": "2019-05-11T11:56:41.736270Z"
    }
   },
   "outputs": [],
   "source": [
    "def frequency(tokens: list):\n",
    "    \"\"\"\n",
    "    Count tokens in text (keys are tokens, values are their corresponding frequencies).\n",
    "    \"\"\"\n",
    "    freq = dict()\n",
    "    for token in tokens:\n",
    "        if token in freq:\n",
    "            freq[token] += 1\n",
    "        else:\n",
    "            freq[token] = 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:56:57.026489Z",
     "start_time": "2019-05-11T11:56:57.016462Z"
    }
   },
   "outputs": [],
   "source": [
    "def represent_text(text, n: int, model: str):\n",
    "    \"\"\"\n",
    "    Extracts all character or word 'n'-grams from a given 'text'.\n",
    "    Any digit is represented through a 0.\n",
    "    Each hyperlink is replaced by an @ sign.\n",
    "    The latter steps are computed through regular expressions.\n",
    "    \"\"\" \n",
    "    if model == 'char-std':\n",
    "\n",
    "        text = regex(text, model)\n",
    "        tokens = [text[i:i+n] for i in range(len(text)-n+1)] \n",
    "\n",
    "        if n == 2:\n",
    "            # create list of unigrams that only consists of punctuation marks\n",
    "            # and extend tokens by that list\n",
    "            punct_unigrams = [token for token in text if not token.isalnum()]\n",
    "            tokens.extend(punct_unigrams)\n",
    "\n",
    "    elif model == 'word':\n",
    "        text = [regex(word, model) for word in text.split() if regex(word, model)]\n",
    "        tokens = [' '.join(text[i:i+n]) for i in range(len(text)-n+1)]\n",
    "\n",
    "    else:\n",
    "        text = regex(text, model)\n",
    "        tokens = tokens = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    \n",
    "    freq = frequency(tokens)\n",
    "\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:56:57.407846Z",
     "start_time": "2019-05-11T11:56:57.401829Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_vocabulary(texts: list, n: int, ft: int, model: str):\n",
    "    \"\"\"\n",
    "    Extracts all character 'n'-grams occurring at least 'ft' times in a set of 'texts'.\n",
    "    \"\"\"\n",
    "    occurrences = {}\n",
    "    \n",
    "    for text in texts:\n",
    "\n",
    "        text_occurrences=represent_text(text, n, model)\n",
    "        \n",
    "        for ngram in text_occurrences.keys():\n",
    "            \n",
    "            if ngram in occurrences:\n",
    "                occurrences[ngram] += text_occurrences[ngram]\n",
    "            else:\n",
    "                occurrences[ngram] = text_occurrences[ngram]\n",
    "    \n",
    "    vocabulary=[]\n",
    "    for i in occurrences.keys():\n",
    "        if occurrences[i] >= ft:\n",
    "            vocabulary.append(i)\n",
    "            \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:56:57.753140Z",
     "start_time": "2019-05-11T11:56:57.749128Z"
    }
   },
   "outputs": [],
   "source": [
    "def extend_vocabulary(n_range: int, n_start: int, texts: list, model: str):\n",
    "    vocab = []\n",
    "    for n in range(n_start, n_range + 1):\n",
    "        n_vocab = extract_vocabulary(texts, n, (n_range - n) + 1, model)\n",
    "        vocab.extend(n_vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T13:29:37.553413Z",
     "start_time": "2019-05-11T13:29:37.503280Z"
    }
   },
   "outputs": [],
   "source": [
    "def baseline(path, outpath, n_start = 2, n_range = 5, pt = 0.1, n_best_factor = 0.7, \n",
    "             lower = False):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    language = []\n",
    "    \n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "            language.append(attrib['language'])\n",
    "                \n",
    "    for index, problem in enumerate(problems):\n",
    "        print(problem)\n",
    "        # Reading information about the problem\n",
    "        infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "        candidates = []\n",
    "        with open(infoproblem, 'r') as f:\n",
    "            fj = json.load(f)\n",
    "            unk_folder = fj['unknown-folder']\n",
    "            for attrib in fj['candidate-authors']:\n",
    "                candidates.append(attrib['author-name'])\n",
    "                \n",
    "        # building training set\n",
    "        train_docs = []\n",
    "        for candidate in candidates:\n",
    "            train_docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "            \n",
    "        train_texts = [text for (text,label) in train_docs]        \n",
    "        train_labels = [label for (text,label) in train_docs]\n",
    "        \n",
    "        # character n-gram vocabulary (syntactical features)\n",
    "        vocab_char_std = extend_vocabulary(n_range, n_start, train_texts, model = 'char-std')\n",
    "        \n",
    "        # character n-gram vocabulary (non-diacrictics / alphabetical symbols are distorted)\n",
    "        vocab_char_dist = extend_vocabulary(n_range, n_start, train_texts, model = 'char-dist')\n",
    "        \n",
    "        # word n-gram vocabulary (content / semantical features)\n",
    "        vocab_word = extend_vocabulary(3, n_start, train_texts, model = 'word')\n",
    "\n",
    "        print('\\t', 'language: ', language[index])\n",
    "        print('\\t', len(candidates), 'candidate authors')\n",
    "        print('\\t', len(train_texts), 'known texts')\n",
    "        \n",
    "        print('\\t', 'word-based vocabulary size:', len(vocab_word))\n",
    "        print('\\t', 'standard character vocabulary size:', len(vocab_char_std))\n",
    "        print('\\t', 'non-alphabetical character vocabulary size:', len(vocab_char_dist))\n",
    "\n",
    "        \n",
    "        # building test set\n",
    "        test_docs = read_files(path+os.sep+problem,unk_folder)\n",
    "        test_texts = [text for (text,label) in test_docs]\n",
    "        \n",
    "        ## initialize tf-idf vectorizer for word n-gram model (captures content) ##\n",
    "        vectorizer_word = TfidfVectorizer(analyzer = 'word', ngram_range = (2, 3), use_idf = True, \n",
    "                                          norm = 'l2', lowercase = lower, vocabulary = vocab_word, \n",
    "                                          smooth_idf = True, sublinear_tf = True)\n",
    "\n",
    "        train_data_word = vectorizer_word.fit_transform(train_texts).toarray()\n",
    "\n",
    "        n_best = int(len(vectorizer_word.idf_) * n_best_factor)\n",
    "        idx_w = np.argsort(vectorizer_word.idf_)[:n_best]\n",
    "\n",
    "        train_data_word = train_data_word[:, idx_w]\n",
    "\n",
    "        test_data_word = vectorizer_word.transform(test_texts).toarray()\n",
    "        test_data_word = test_data_word[:, idx_w]\n",
    "        \n",
    "        ## initialize tf-idf vectorizer for char n-gram model in which non-diacritics are distorted ##\n",
    "        \n",
    "        vectorizer_char_dist = TfidfVectorizer(analyzer = 'char', ngram_range = (2, n_range), use_idf = True, \n",
    "                                     norm = 'l2', lowercase = lower, vocabulary = vocab_char_dist, \n",
    "                                     min_df = 0.2, max_df = 0.8, smooth_idf = True, \n",
    "                                     sublinear_tf = True)\n",
    "\n",
    "        train_data_char_dist = vectorizer_char_dist.fit_transform(train_texts).toarray()\n",
    "\n",
    "        n_best = int(len(vectorizer_char_dist.idf_) * n_best_factor)\n",
    "        idx_c = np.argsort(vectorizer_char_dist.idf_)[:n_best]\n",
    "\n",
    "        train_data_char_dist = train_data_char_dist[:, idx_c]\n",
    "\n",
    "        test_data_char_dist = vectorizer_char_dist.transform(test_texts).toarray()\n",
    "        test_data_char_dist = test_data_char_dist[:, idx_c]\n",
    "        \n",
    "        ##  initialize tf-idf vectorizer for char n-gram model (captures syntactical features) ##\n",
    "        vectorizer_char_std = TfidfVectorizer(analyzer = 'char', ngram_range = (2, n_range), use_idf = True, \n",
    "                                     norm = 'l2', lowercase = lower, vocabulary = vocab_char_std, \n",
    "                                     min_df = 0.2, max_df = 0.8, smooth_idf = True, \n",
    "                                     sublinear_tf = True)\n",
    "\n",
    "        train_data_char_std = vectorizer_char_std.fit_transform(train_texts).toarray()\n",
    "\n",
    "        n_best = int(len(vectorizer_char_std.idf_) * n_best_factor)\n",
    "        idx_c = np.argsort(vectorizer_char_std.idf_)[:n_best]\n",
    "\n",
    "        train_data_char_std = train_data_char_std[:, idx_c]\n",
    "\n",
    "        test_data_char_std = vectorizer_char_std.transform(test_texts).toarray()\n",
    "        test_data_char_std = test_data_char_std[:, idx_c]\n",
    "        \n",
    "        print('\\t', len(test_texts), 'unknown texts')\n",
    "        \n",
    "        max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "        \n",
    "        ## scale text data for word n-gram model ##\n",
    "        scaled_train_data_word = max_abs_scaler.fit_transform(train_data_word)\n",
    "        scaled_test_data_word = max_abs_scaler.transform(test_data_word)\n",
    "        \n",
    "        ## scale text data for char dist n-gram model ##\n",
    "        scaled_train_data_char_dist = max_abs_scaler.fit_transform(train_data_char_dist)\n",
    "        scaled_test_data_char_dist = max_abs_scaler.transform(test_data_char_dist)\n",
    "        \n",
    "         ## scale text data for char std n-gram model ##\n",
    "        scaled_train_data_char_std = max_abs_scaler.fit_transform(train_data_char_std)\n",
    "        scaled_test_data_char_std = max_abs_scaler.transform(test_data_char_std)\n",
    "        \n",
    "        # initialize truncated singular value decomposition\n",
    "        #svd = TruncatedSVD(n_components = 63, algorithm = 'randomized', random_state = 42)\n",
    "        #scaled_train_data_char_std = svd.fit_transform(scaled_train_data_char_std)\n",
    "        #scaled_test_data_char_std = svd.transform(scaled_test_data_char_std)\n",
    "        \n",
    "        # model\n",
    "        clf=CalibratedClassifierCV(OneVsRestClassifier(SVC(C=1, gamma = 'auto')))\n",
    "        clf.fit(scaled_train_data_char_std, train_labels)\n",
    "        predictions=clf.predict(scaled_test_data_char_std)\n",
    "        proba=clf.predict_proba(scaled_test_data_char_std)\n",
    "        \n",
    "        # Reject option (used in open-set cases)\n",
    "        count=0\n",
    "        for i,p in enumerate(predictions):\n",
    "            sproba=sorted(proba[i],reverse=True)\n",
    "            if sproba[0]-sproba[1]<pt:\n",
    "                predictions[i]=u'<UNK>'\n",
    "                count=count+1\n",
    "        print('\\t',count,'texts left unattributed')\n",
    "        \n",
    "        # Saving output data\n",
    "        out_data=[]\n",
    "        unk_filelist = glob.glob(path+os.sep+problem+os.sep+unk_folder+os.sep+'*.txt')\n",
    "        pathlen=len(path+os.sep+problem+os.sep+unk_folder+os.sep)\n",
    "        for i,v in enumerate(predictions):\n",
    "            out_data.append({'unknown-text': unk_filelist[i][pathlen:], 'predicted-author': v})\n",
    "        with open(outpath+os.sep+'answers-'+problem+'.json', 'w') as f:\n",
    "            json.dump(out_data, f, indent=4)\n",
    "        print('\\t', 'answers saved to file','answers-'+problem+'.json')\n",
    "    print('elapsed time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-11T13:29:38.324Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t word-based vocabulary size: 51528\n",
      "\t standard character vocabulary size: 85703\n",
      "\t non-alphabetical character vocabulary size: 2762\n",
      "\t 561 unknown texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Continuum\\anaconda3\\envs\\superdynet\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "baseline(cwd + \"\\\\cross-domain-authorship-attribution-train\", cwd + '\\\\answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T12:14:01.152450Z",
     "start_time": "2019-05-11T12:13:46.391Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class RNNLanguageModel:\n",
    "    def __init__(self, model, LAYERS, INPUT_DIM, HIDDEN_DIM, VOCAB_SIZE, builder = dy.SimpleRNNBuilder):\n",
    "        \n",
    "        self.builder = builder(LAYERS, INPUT_DIM, HIDDEN_DIM, model)\n",
    "        self.lookup = model.add_lookup_parameters((VOCAB_SIZE, INPUT_DIM), name=\"lookup\")\n",
    "        self.R = model.add_parameters((VOCAB_SIZE, HIDDEN_DIM), name=\"hidden2out\")\n",
    "        self.bias = model.add_parameters((VOCAB_SIZE), name=\"bias\")\n",
    "\n",
    "    def save_to_disk(self, filename):\n",
    "        dy.save(filename, [self.builder, self.lookup, self.R, self.bias])\n",
    "\n",
    "    def load_from_disk(self, filename):\n",
    "        (self.builder, self.lookup, self.R, self.bias) = dy.load(filename, model)\n",
    "        \n",
    "    def build_lm_graph(self, sent):\n",
    "        dy.renew_cg()\n",
    "        init_state = self.builder.initial_state()\n",
    "\n",
    "        errs = [] # will hold expressions\n",
    "        es=[]\n",
    "        state = init_state\n",
    "        for (cw,nw) in zip(sent,sent[1:]):\n",
    "            # assume word is already a word-id\n",
    "            x_t = dy.lookup(self.lookup, int(cw))\n",
    "            state = state.add_input(x_t)\n",
    "            y_t = state.output()\n",
    "            r_t = self.bias + (self.R * y_t)\n",
    "            err = dy.pickneglogsoftmax(r_t, int(nw))\n",
    "            errs.append(err)\n",
    "        nerr = dy.esum(errs)\n",
    "        return nerr\n",
    "    \n",
    "    def predict_next_word(self, sentence):\n",
    "        dy.renew_cg()\n",
    "        init_state = self.builder.initial_state()\n",
    "        state = init_state\n",
    "        for cw in sentence:\n",
    "            # assume word is already a word-id\n",
    "            x_t = self.lookup[int(cw)]\n",
    "            state = state.add_input(x_t)\n",
    "        y_t = state.output()\n",
    "        r_t = self.bias + (self.R * y_t)\n",
    "        prob = dy.softmax(r_t)\n",
    "        return prob\n",
    "        \n",
    "    def sample(self, first=1, nchars=0, stop=-1):\n",
    "        res = [first]\n",
    "        dy.renew_cg()\n",
    "        state = self.builder.initial_state()\n",
    "\n",
    "        cw = first\n",
    "        while True:\n",
    "            x_t = self.lookup[cw]\n",
    "            state = state.add_input(x_t)\n",
    "            y_t = state.output()\n",
    "            r_t = self.bias + (self.R * y_t)\n",
    "            ydist = dy.softmax(r_t)\n",
    "            dist = ydist.vec_value()\n",
    "            rnd = random.random()\n",
    "            for i,p in enumerate(dist):\n",
    "                rnd -= p\n",
    "                if rnd <= 0: break\n",
    "            res.append(i)\n",
    "            cw = i\n",
    "            if cw == stop: break\n",
    "            if nchars and len(res) > nchars: break\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = \"allnames.txt\"\n",
    "\n",
    "LAYERS = 2\n",
    "INPUT_DIM = 32 #50  #256\n",
    "HIDDEN_DIM = 128 # 50  #1024\n",
    "\n",
    "train = util.CharsCorpusReader(corpus, begin=\"<s>\")\n",
    "vocab = util.Vocab.from_corpus(train)\n",
    "\n",
    "VOCAB_SIZE = vocab.size()\n",
    "\n",
    "model = dy.Model()\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.2)\n",
    "\n",
    "lm = RNNLanguageModel(model, LAYERS, INPUT_DIM, HIDDEN_DIM, VOCAB_SIZE, builder=dy.SimpleRNNBuilder)\n",
    "#lm = RNNLanguageModel(model, LAYERS, INPUT_DIM, HIDDEN_DIM, VOCAB_SIZE, builder=dy.LSTMBuilder)\n",
    "\n",
    "\n",
    "train = list(train)\n",
    "\n",
    "losses = []\n",
    "\n",
    "chars = loss = 0.0\n",
    "\n",
    "for ITER in range(3):\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    for i,sent in enumerate(train):\n",
    "        _start = time.time()\n",
    "        \n",
    "        if i % 2500 == 0:\n",
    "            trainer.status()\n",
    "            print (i,len(train))\n",
    "            \n",
    "            if chars > 0: print(loss / chars,)\n",
    "                \n",
    "            for _ in range(1):\n",
    "                samp = lm.sample(first=vocab.w2i[\"<s>\"],stop=vocab.w2i[\"\\n\"])\n",
    "                print(\"\".join([vocab.i2w[c] for c in samp]).strip())\n",
    "            loss = 0.0\n",
    "            chars = 0.0\n",
    "\n",
    "        chars += len(sent)-1\n",
    "        isent = [vocab.w2i[w] for w in sent]\n",
    "        errs = lm.build_lm_graph(isent)\n",
    "        loss += errs.scalar_value()\n",
    "        errs.backward()\n",
    "        trainer.update()\n",
    "    print (\"TM:\",(time.time() - _start)/len(sent))\n",
    "    print(\"ITER {}, loss={}\".format(ITER, loss))\n",
    "    losses.append(loss)\n",
    "    trainer.status()\n",
    "\n",
    "lm.save_to_disk(\"RNNLanguageModel.model\")\n",
    "\n",
    "print(\"loading the saved model...\")\n",
    "lm.load_from_disk(\"RNNLanguageModel.model\")\n",
    "samp = lm.sample(first=vocab.w2i[\"<s>\"],stop=vocab.w2i[\"\\n\"])\n",
    "print(\"\".join([vocab.i2w[c] for c in samp]).strip())\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

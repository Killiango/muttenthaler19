{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T21:37:22.875478Z",
     "start_time": "2019-05-08T21:37:19.387511Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T21:37:35.757908Z",
     "start_time": "2019-05-08T21:37:35.754178Z"
    }
   },
   "outputs": [],
   "source": [
    "# get current directory\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T21:37:36.329330Z",
     "start_time": "2019-05-08T21:37:36.320258Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_files(path: str, label: str):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(path+os.sep+label+os.sep+'*.txt')\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label))\n",
    "        f.close()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T21:37:36.974822Z",
     "start_time": "2019-05-08T21:37:36.961280Z"
    }
   },
   "outputs": [],
   "source": [
    "def regex(string: str, words):\n",
    "    \n",
    "    string = re.sub(\"[0-9]\", \"0\", string) # each digit will be represented as a 0\n",
    "    string = re.sub(r'( \\n| \\t)+', '', string)\n",
    "    #text = re.sub(\"[0-9]+(([.,^])[0-9]+)?\", \"#\", text)\n",
    "    string = re.sub(\"https:\\\\\\+([a-zA-Z0-9.]+)?\", \"@\", string)\n",
    "    \n",
    "    if words:\n",
    "        # if text will be represented as word n-gram bag-of-words, remove all punctuation\n",
    "        string = ''.join([char for char in string if char.isalnum()])\n",
    "        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T21:37:37.494931Z",
     "start_time": "2019-05-08T21:37:37.471907Z"
    }
   },
   "outputs": [],
   "source": [
    "def represent_text(text, n: int, words: bool):\n",
    "    \"\"\"\n",
    "    Extracts all character 'n'-grams from a given 'text'.\n",
    "    Each digit is represented as a hashtag symbol (#) which in general denotes any number.\n",
    "    Each hyperlink is replaced by an @ sign.\n",
    "    The latter steps are computed through regular expressions.\n",
    "    \"\"\"    \n",
    "    if not words:\n",
    "\n",
    "        text = regex(text, words)\n",
    "        tokens = [text[i:i+n] for i in range(len(text)-n+1)] \n",
    "\n",
    "        if n == 2:\n",
    "            # create list of unigrams that only consists of punctuation marks\n",
    "            punct_unigrams = [token for token in text if token.isalnum() == False]\n",
    "            tokens.extend(punct_unigrams)\n",
    "\n",
    "    else:\n",
    "        text = [regex(word, words) for word in text.split() if regex(word, words)]\n",
    "        tokens = [' '.join(text[i:i+n]) for i in range(len(text)-n+1)]\n",
    "\n",
    "    # create frequency text representation (keys are tokens, values are their corresponding frequencies)\n",
    "    frequency = {token: tokens.count(token) for token in list(set(tokens))}\n",
    "\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T21:37:37.953411Z",
     "start_time": "2019-05-08T21:37:37.936374Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_vocabulary(texts: list, n: int, ft: int, words: bool):\n",
    "    \n",
    "    # Extracts all character 'n'-grams occurring at least 'ft' times in a set of 'texts'\n",
    "    occurrences=defaultdict(int)\n",
    "    \n",
    "    for text in texts:\n",
    "\n",
    "        text_occurrences=represent_text(text, n, words)\n",
    "        \n",
    "        for ngram in text_occurrences.keys():\n",
    "            \n",
    "            if ngram in occurrences:\n",
    "                occurrences[ngram] += text_occurrences[ngram]\n",
    "            else:\n",
    "                occurrences[ngram] = text_occurrences[ngram]\n",
    "    \n",
    "    vocabulary=[]\n",
    "    for i in occurrences.keys():\n",
    "        if occurrences[i] >= ft:\n",
    "            vocabulary.append(i)\n",
    "            \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T21:37:38.383497Z",
     "start_time": "2019-05-08T21:37:38.376720Z"
    }
   },
   "outputs": [],
   "source": [
    "def extend_vocabulary(n_range: int, n_start: int, texts: list, words: bool):\n",
    "    vocab = []\n",
    "    for n in range(n_start, n_range + 1):\n",
    "        n_vocab = extract_vocabulary(texts, n, (n_range - n) + 1, words)\n",
    "        vocab.extend(n_vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T21:37:38.933051Z",
     "start_time": "2019-05-08T21:37:38.887355Z"
    }
   },
   "outputs": [],
   "source": [
    "def baseline(path, outpath, n_start = 2, n_range = 4, pt = 0.1, n_best_factor = 0.5, lower = False, words = False):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    language = []\n",
    "    \n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "            language.append(attrib['language'])\n",
    "                \n",
    "    for index, problem in enumerate(problems):\n",
    "        print(problem)\n",
    "        # Reading information about the problem\n",
    "        infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "        candidates = []\n",
    "        with open(infoproblem, 'r') as f:\n",
    "            fj = json.load(f)\n",
    "            unk_folder = fj['unknown-folder']\n",
    "            for attrib in fj['candidate-authors']:\n",
    "                candidates.append(attrib['author-name'])\n",
    "                \n",
    "        # building training set\n",
    "        train_docs = []\n",
    "        for candidate in candidates:\n",
    "            train_docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "            \n",
    "        train_texts = [text for (text,label) in train_docs]        \n",
    "        train_labels = [label for (text,label) in train_docs]\n",
    "        \n",
    "        # character n-gram vocabulary\n",
    "        vocab_char = extend_vocabulary(n_range, n_start, train_texts, words = False)\n",
    "        \n",
    "        # word n-gram vocabulary\n",
    "        vocab_word = extend_vocabulary(3, n_start, train_texts, words = True)\n",
    "\n",
    "        print('\\t', 'language: ', language[index])\n",
    "        print('\\t', len(candidates), 'candidate authors')\n",
    "        print('\\t', len(train_texts), 'known texts')\n",
    "        \n",
    "        if words:\n",
    "            print('\\t', 'vocabulary size:', len(vocab_word))\n",
    "        else:\n",
    "            print('\\t', 'vocabulary size:', len(vocab_char))\n",
    "       \n",
    "        \n",
    "        # building test set\n",
    "        test_docs = read_files(path+os.sep+problem,unk_folder)\n",
    "        test_texts = [text for (text,label) in test_docs]\n",
    "        \n",
    "        if words:\n",
    "            # initiialize tf-idf-vectorizer for word n-gram bag-of-words\n",
    "            vect = TfidfVectorizer(analyzer = 'word', ngram_range = (2, 3), use_idf = True, norm = 'l2', \n",
    "                                   lowercase = lower, vocabulary = vocab_word, smooth_idf = True, sublinear_tf = True)\n",
    "                    \n",
    "            train_data_word = vect.fit_transform(train_texts).toarray()\n",
    "            \n",
    "            n_best = int(len(vect.idf_) * n_best_factor)\n",
    "            idx_w = np.argsort(vect.idf_)[:n_best]\n",
    "            \n",
    "            train_data = train_data_word[:, idx_w]\n",
    "            \n",
    "            test_data_word = vect.transform(test_texts).toarray()\n",
    "            test_data = test_data_word[:, idx_w]\n",
    "        \n",
    "        else:\n",
    "            # initialize tf-idf-vectorizer for character n-gram bag-of-words\n",
    "            vectorizer = TfidfVectorizer(analyzer = 'char', ngram_range = (2, n_range), use_idf = True, norm = 'l2', \n",
    "                                         lowercase = lower, vocabulary = vocab_char, min_df = 0.2, max_df = 0.8, \n",
    "                                         smooth_idf = True, sublinear_tf = True)\n",
    "        \n",
    "            train_data_char = vectorizer.fit_transform(train_texts).toarray()\n",
    "            \n",
    "            n_best = int(len(vectorizer.idf_) * n_best_factor)\n",
    "            idx_c = np.argsort(vectorizer.idf_)[:n_best]\n",
    "            \n",
    "            train_data = train_data_char[:, idx_c]\n",
    "            \n",
    "            test_data_char = vectorizer.transform(test_texts).toarray()\n",
    "            test_data = test_data_char[:, idx_c]\n",
    "        \n",
    "        print('\\t', len(test_texts), 'unknown texts')\n",
    "        \n",
    "        # Applying SVM\n",
    "        max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "        scaled_train_data = max_abs_scaler.fit_transform(train_data)\n",
    "        scaled_test_data = max_abs_scaler.transform(test_data)\n",
    "        \n",
    "        # initialize truncated singular value decomposition\n",
    "        #svd = TruncatedSVD(n_components = 63, algorithm = 'randomized', random_state = 42)\n",
    "        #scaled_train_data = svd.fit_transform(scaled_train_data)\n",
    "        #scaled_test_data = svd.transform(scaled_test_data)\n",
    "        \n",
    "        # model \n",
    "        clf=CalibratedClassifierCV(OneVsRestClassifier(SVC(C=1, gamma = 'auto')))\n",
    "        clf.fit(scaled_train_data, train_labels)\n",
    "        predictions=clf.predict(scaled_test_data)\n",
    "        proba=clf.predict_proba(scaled_test_data)\n",
    "        \n",
    "        # Reject option (used in open-set cases)\n",
    "        count=0\n",
    "        for i,p in enumerate(predictions):\n",
    "            sproba=sorted(proba[i],reverse=True)\n",
    "            if sproba[0]-sproba[1]<pt:\n",
    "                predictions[i]=u'<UNK>'\n",
    "                count=count+1\n",
    "        print('\\t',count,'texts left unattributed')\n",
    "        \n",
    "        # Saving output data\n",
    "        out_data=[]\n",
    "        unk_filelist = glob.glob(path+os.sep+problem+os.sep+unk_folder+os.sep+'*.txt')\n",
    "        pathlen=len(path+os.sep+problem+os.sep+unk_folder+os.sep)\n",
    "        for i,v in enumerate(predictions):\n",
    "            out_data.append({'unknown-text': unk_filelist[i][pathlen:], 'predicted-author': v})\n",
    "        with open(outpath+os.sep+'answers-'+problem+'.json', 'w') as f:\n",
    "            json.dump(out_data, f, indent=4)\n",
    "        print('\\t', 'answers saved to file','answers-'+problem+'.json')\n",
    "    print('elapsed time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T21:47:10.428372Z",
     "start_time": "2019-05-08T21:37:39.445779Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 35317\n",
      "\t 561 unknown texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 109 texts left unattributed\n",
      "\t answers saved to file answers-problem00001.json\n",
      "problem00002\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 34454\n",
      "\t 137 unknown texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 52 texts left unattributed\n",
      "\t answers saved to file answers-problem00002.json\n",
      "problem00003\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 34203\n",
      "\t 211 unknown texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 113 texts left unattributed\n",
      "\t answers saved to file answers-problem00003.json\n",
      "problem00004\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 36655\n",
      "\t 273 unknown texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 131 texts left unattributed\n",
      "\t answers saved to file answers-problem00004.json\n",
      "problem00005\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 33178\n",
      "\t 264 unknown texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 85 texts left unattributed\n",
      "\t answers saved to file answers-problem00005.json\n",
      "elapsed time: 570.8993763923645\n"
     ]
    }
   ],
   "source": [
    "baseline(cwd + \"\\\\cross-domain-authorship-attribution-train\", cwd + '\\\\answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class RNNLanguageModel:\n",
    "    def __init__(self, model, LAYERS, INPUT_DIM, HIDDEN_DIM, VOCAB_SIZE, builder = dy.SimpleRNNBuilder):\n",
    "        \n",
    "        self.builder = builder(LAYERS, INPUT_DIM, HIDDEN_DIM, model)\n",
    "        self.lookup = model.add_lookup_parameters((VOCAB_SIZE, INPUT_DIM), name=\"lookup\")\n",
    "        self.R = model.add_parameters((VOCAB_SIZE, HIDDEN_DIM), name=\"hidden2out\")\n",
    "        self.bias = model.add_parameters((VOCAB_SIZE), name=\"bias\")\n",
    "\n",
    "    def save_to_disk(self, filename):\n",
    "        dy.save(filename, [self.builder, self.lookup, self.R, self.bias])\n",
    "\n",
    "    def load_from_disk(self, filename):\n",
    "        (self.builder, self.lookup, self.R, self.bias) = dy.load(filename, model)\n",
    "        \n",
    "    def build_lm_graph(self, sent):\n",
    "        dy.renew_cg()\n",
    "        init_state = self.builder.initial_state()\n",
    "\n",
    "        errs = [] # will hold expressions\n",
    "        es=[]\n",
    "        state = init_state\n",
    "        for (cw,nw) in zip(sent,sent[1:]):\n",
    "            # assume word is already a word-id\n",
    "            x_t = dy.lookup(self.lookup, int(cw))\n",
    "            state = state.add_input(x_t)\n",
    "            y_t = state.output()\n",
    "            r_t = self.bias + (self.R * y_t)\n",
    "            err = dy.pickneglogsoftmax(r_t, int(nw))\n",
    "            errs.append(err)\n",
    "        nerr = dy.esum(errs)\n",
    "        return nerr\n",
    "    \n",
    "    def predict_next_word(self, sentence):\n",
    "        dy.renew_cg()\n",
    "        init_state = self.builder.initial_state()\n",
    "        state = init_state\n",
    "        for cw in sentence:\n",
    "            # assume word is already a word-id\n",
    "            x_t = self.lookup[int(cw)]\n",
    "            state = state.add_input(x_t)\n",
    "        y_t = state.output()\n",
    "        r_t = self.bias + (self.R * y_t)\n",
    "        prob = dy.softmax(r_t)\n",
    "        return prob\n",
    "        \n",
    "    def sample(self, first=1, nchars=0, stop=-1):\n",
    "        res = [first]\n",
    "        dy.renew_cg()\n",
    "        state = self.builder.initial_state()\n",
    "\n",
    "        cw = first\n",
    "        while True:\n",
    "            x_t = self.lookup[cw]\n",
    "            state = state.add_input(x_t)\n",
    "            y_t = state.output()\n",
    "            r_t = self.bias + (self.R * y_t)\n",
    "            ydist = dy.softmax(r_t)\n",
    "            dist = ydist.vec_value()\n",
    "            rnd = random.random()\n",
    "            for i,p in enumerate(dist):\n",
    "                rnd -= p\n",
    "                if rnd <= 0: break\n",
    "            res.append(i)\n",
    "            cw = i\n",
    "            if cw == stop: break\n",
    "            if nchars and len(res) > nchars: break\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = \"allnames.txt\"\n",
    "\n",
    "LAYERS = 2\n",
    "INPUT_DIM = 32 #50  #256\n",
    "HIDDEN_DIM = 128 # 50  #1024\n",
    "\n",
    "train = util.CharsCorpusReader(corpus, begin=\"<s>\")\n",
    "vocab = util.Vocab.from_corpus(train)\n",
    "\n",
    "VOCAB_SIZE = vocab.size()\n",
    "\n",
    "model = dy.Model()\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.2)\n",
    "\n",
    "lm = RNNLanguageModel(model, LAYERS, INPUT_DIM, HIDDEN_DIM, VOCAB_SIZE, builder=dy.SimpleRNNBuilder)\n",
    "#lm = RNNLanguageModel(model, LAYERS, INPUT_DIM, HIDDEN_DIM, VOCAB_SIZE, builder=dy.LSTMBuilder)\n",
    "\n",
    "\n",
    "train = list(train)\n",
    "\n",
    "losses = []\n",
    "\n",
    "chars = loss = 0.0\n",
    "\n",
    "for ITER in range(3):\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    for i,sent in enumerate(train):\n",
    "        _start = time.time()\n",
    "        \n",
    "        if i % 2500 == 0:\n",
    "            trainer.status()\n",
    "            print (i,len(train))\n",
    "            \n",
    "            if chars > 0: print(loss / chars,)\n",
    "                \n",
    "            for _ in range(1):\n",
    "                samp = lm.sample(first=vocab.w2i[\"<s>\"],stop=vocab.w2i[\"\\n\"])\n",
    "                print(\"\".join([vocab.i2w[c] for c in samp]).strip())\n",
    "            loss = 0.0\n",
    "            chars = 0.0\n",
    "\n",
    "        chars += len(sent)-1\n",
    "        isent = [vocab.w2i[w] for w in sent]\n",
    "        errs = lm.build_lm_graph(isent)\n",
    "        loss += errs.scalar_value()\n",
    "        errs.backward()\n",
    "        trainer.update()\n",
    "    print (\"TM:\",(time.time() - _start)/len(sent))\n",
    "    print(\"ITER {}, loss={}\".format(ITER, loss))\n",
    "    losses.append(loss)\n",
    "    trainer.status()\n",
    "\n",
    "lm.save_to_disk(\"RNNLanguageModel.model\")\n",
    "\n",
    "print(\"loading the saved model...\")\n",
    "lm.load_from_disk(\"RNNLanguageModel.model\")\n",
    "samp = lm.sample(first=vocab.w2i[\"<s>\"],stop=vocab.w2i[\"\\n\"])\n",
    "print(\"\".join([vocab.i2w[c] for c in samp]).strip())\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

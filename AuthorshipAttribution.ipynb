{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T12:43:38.389118Z",
     "start_time": "2019-05-08T12:43:30.875772Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T12:52:22.253812Z",
     "start_time": "2019-05-08T12:52:22.250805Z"
    }
   },
   "outputs": [],
   "source": [
    "# get current directory\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T12:52:23.435329Z",
     "start_time": "2019-05-08T12:52:23.430316Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_files(path: str, label: str):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(path+os.sep+label+os.sep+'*.txt')\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label))\n",
    "        f.close()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T12:52:23.956426Z",
     "start_time": "2019-05-08T12:52:23.950411Z"
    }
   },
   "outputs": [],
   "source": [
    "def represent_text(text, n: int):\n",
    "    \"\"\"\n",
    "    Extracts all character 'n'-grams from a given 'text'.\n",
    "    Each digit is represented as a hashtag symbol (#) which in general denotes any number.\n",
    "    Each hyperlink is replaced by an @ sign.\n",
    "    The latter steps are computed through regular expressions.\n",
    "    \"\"\"    \n",
    "    if n > 0:\n",
    "        text = re.sub(\"[0-9]\", \"0\", text) # each digit will be represented as a 0\n",
    "        text = re.sub(r'( \\n| \\t)+', '', text)\n",
    "        #text = re.sub(\"[0-9]+(([.,^])[0-9]+)?\", \"#\", text)\n",
    "        text = re.sub(\"https:\\\\\\+([a-zA-Z0-9.]+)?\", \"@\", text)\n",
    "        tokens = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "        \n",
    "        if n == 2:\n",
    "            # create list of unigrams that only consists of punctuation marks\n",
    "            punct_unigrams = [token for token in text if token.isalnum() == False]\n",
    "            tokens.extend(punct_unigrams)\n",
    "            \n",
    "    # create frequency text representation (keys are tokens, values are their corresponding frequencies)\n",
    "    frequency = {token: tokens.count(token) for token in list(set(tokens))}\n",
    "        \n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T12:52:24.414276Z",
     "start_time": "2019-05-08T12:52:24.405251Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_vocabulary(texts, n: int, ft = 3):\n",
    "    \n",
    "    # Extracts all characer 'n'-grams occurring at least 'ft' times in a set of 'texts'\n",
    "    occurrences=defaultdict(int)\n",
    "    \n",
    "    for text in texts:\n",
    "        \n",
    "        text_occurrences=represent_text(text,n)\n",
    "        \n",
    "        for ngram in text_occurrences.keys():\n",
    "            \n",
    "            if ngram in occurrences:\n",
    "                occurrences[ngram] += text_occurrences[ngram]\n",
    "            else:\n",
    "                occurrences[ngram] = text_occurrences[ngram]\n",
    "    \n",
    "    vocabulary=[]\n",
    "    for i in occurrences.keys():\n",
    "        if occurrences[i] >= ft:\n",
    "            vocabulary.append(i)\n",
    "            \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T13:00:33.964145Z",
     "start_time": "2019-05-08T13:00:33.934065Z"
    }
   },
   "outputs": [],
   "source": [
    "def baseline(path, outpath, n_range = 4, pt = 0.1, lower = False, words = False):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    language = []\n",
    "    \n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "            language.append(attrib['language'])\n",
    "            \n",
    "    stopWords = list(set(stopwords.words('english')))\n",
    "    \n",
    "    for index, problem in enumerate(problems):\n",
    "        print(problem)\n",
    "        # Reading information about the problem\n",
    "        infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "        candidates = []\n",
    "        with open(infoproblem, 'r') as f:\n",
    "            fj = json.load(f)\n",
    "            unk_folder = fj['unknown-folder']\n",
    "            for attrib in fj['candidate-authors']:\n",
    "                candidates.append(attrib['author-name'])\n",
    "                \n",
    "        # Building training set\n",
    "        train_docs = []\n",
    "        for candidate in candidates:\n",
    "            train_docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "            \n",
    "        train_texts = [text for i,(text,label) in enumerate(train_docs)]\n",
    "        train_labels = [label for i,(text,label) in enumerate(train_docs)]\n",
    "        \n",
    "        vocab = []\n",
    "        for n in range(2, n_range + 1):\n",
    "            n_vocab = extract_vocabulary(train_texts, n, (n_range - n) + 1)\n",
    "            vocab.extend(n_vocab)\n",
    "        \n",
    "        # initialize tf-idf-vectorizer\n",
    "        vectorizer = TfidfVectorizer(analyzer = 'char', ngram_range = (2, n_range), use_idf = True, norm = 'l2', lowercase = lower, vocabulary = vocab, min_df = 0.2, max_df = 0.8)\n",
    "        train_data_char = vectorizer.fit_transform(train_texts).toarray()\n",
    "        \n",
    "        print(train_data_char.shape)\n",
    "    \n",
    "        indexes = np.argsort(vectorizer.idf_)[:12000]\n",
    "        train_data_char = train_data_char[:, indexes]\n",
    "        \n",
    "        print(train_data_char.shape)\n",
    "        \n",
    "        # initialize truncated singular value decomposition\n",
    "        # svd = TruncatedSVD(n_components = 50, algorithm = 'randomized', random_state = 42)\n",
    "        # train_data_lsa = svd.fit_transform(train_data)\n",
    "        \n",
    "        # print(train_data_lsa.shape)\n",
    "        \n",
    "        print('\\t', 'language: ', language[index])\n",
    "        print('\\t', len(candidates), 'candidate authors')\n",
    "        print('\\t', len(train_texts), 'known texts')\n",
    "        print('\\t', 'vocabulary size:', len(vocab))\n",
    "        \n",
    "        # Building test set\n",
    "        test_docs = read_files(path+os.sep+problem,unk_folder)\n",
    "        test_texts = [text for i,(text,label) in enumerate(test_docs)]\n",
    "        \n",
    "        test_data_char = vectorizer.transform(test_texts).toarray()\n",
    "        test_data_char = test_data_char[:, indexes]\n",
    "        \n",
    "        print(test_data_char.shape)\n",
    "        \n",
    "        if words == True:\n",
    "            vect = CountVectorizer(analyzer = 'word', ngram_range = (1, 1), lowercase = lower, vocabulary = stopWords)\n",
    "            train_data_word = vect.fit_transform(train_texts).toarray()\n",
    "            test_data_word = vect.transform(test_texts).toarray()\n",
    "            train_data, test_data = np.hstack((train_data_char, train_data_word)), np.hstack((test_data_char, test_data_word))\n",
    "        \n",
    "        else:\n",
    "            train_data, test_data = train_data_char, test_data_char\n",
    "            \n",
    "        print(train_data.shape)\n",
    "        print()\n",
    "        print(test_data.shape)\n",
    "        \n",
    "        #test_data_lsa = svd.transform(test_data)\n",
    "        \n",
    "        # Applying SVM\n",
    "        max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "        scaled_train_data = max_abs_scaler.fit_transform(train_data) #train_data_lsa\n",
    "        scaled_test_data = max_abs_scaler.transform(test_data) #train_data_lsa\n",
    "        \n",
    "        # model \n",
    "        clf=CalibratedClassifierCV(OneVsRestClassifier(SVC(C=1, gamma = 'auto')))\n",
    "        clf.fit(scaled_train_data, train_labels)\n",
    "        predictions=clf.predict(scaled_test_data)\n",
    "        proba=clf.predict_proba(scaled_test_data)\n",
    "        \n",
    "        # Reject option (used in open-set cases)\n",
    "        count=0\n",
    "        for i,p in enumerate(predictions):\n",
    "            sproba=sorted(proba[i],reverse=True)\n",
    "            if sproba[0]-sproba[1]<pt:\n",
    "                predictions[i]=u'<UNK>'\n",
    "                count=count+1\n",
    "        print('\\t',count,'texts left unattributed')\n",
    "        \n",
    "        # Saving output data\n",
    "        out_data=[]\n",
    "        unk_filelist = glob.glob(path+os.sep+problem+os.sep+unk_folder+os.sep+'*.txt')\n",
    "        pathlen=len(path+os.sep+problem+os.sep+unk_folder+os.sep)\n",
    "        for i,v in enumerate(predictions):\n",
    "            out_data.append({'unknown-text': unk_filelist[i][pathlen:], 'predicted-author': v})\n",
    "        with open(outpath+os.sep+'answers-'+problem+'.json', 'w') as f:\n",
    "            json.dump(out_data, f, indent=4)\n",
    "        print('\\t', 'answers saved to file','answers-'+problem+'.json')\n",
    "    print('elapsed time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T13:03:58.590329Z",
     "start_time": "2019-05-08T13:00:34.389619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "(63, 35317)\n",
      "(63, 12000)\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 35317\n",
      "(561, 12000)\n",
      "(63, 12179)\n",
      "\n",
      "(561, 12179)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 105 texts left unattributed\n",
      "\t answers saved to file answers-problem00001.json\n",
      "problem00002\n",
      "(63, 34454)\n",
      "(63, 12000)\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 34454\n",
      "(137, 12000)\n",
      "(63, 12179)\n",
      "\n",
      "(137, 12179)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 57 texts left unattributed\n",
      "\t answers saved to file answers-problem00002.json\n",
      "problem00003\n",
      "(63, 34203)\n",
      "(63, 12000)\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 34203\n",
      "(211, 12000)\n",
      "(63, 12179)\n",
      "\n",
      "(211, 12179)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 105 texts left unattributed\n",
      "\t answers saved to file answers-problem00003.json\n",
      "problem00004\n",
      "(63, 36655)\n",
      "(63, 12000)\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 36655\n",
      "(273, 12000)\n",
      "(63, 12179)\n",
      "\n",
      "(273, 12179)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 144 texts left unattributed\n",
      "\t answers saved to file answers-problem00004.json\n",
      "problem00005\n",
      "(63, 33178)\n",
      "(63, 12000)\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 33178\n",
      "(264, 12000)\n",
      "(63, 12179)\n",
      "\n",
      "(264, 12179)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 88 texts left unattributed\n",
      "\t answers saved to file answers-problem00005.json\n",
      "elapsed time: 204.1756820678711\n"
     ]
    }
   ],
   "source": [
    "baseline(cwd + \"\\\\cross-domain-authorship-attribution-train\", cwd + '\\\\answers', words = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class RNNLanguageModel:\n",
    "    def __init__(self, model, LAYERS, INPUT_DIM, HIDDEN_DIM, VOCAB_SIZE, builder = dy.SimpleRNNBuilder):\n",
    "        \n",
    "        self.builder = builder(LAYERS, INPUT_DIM, HIDDEN_DIM, model)\n",
    "        self.lookup = model.add_lookup_parameters((VOCAB_SIZE, INPUT_DIM), name=\"lookup\")\n",
    "        self.R = model.add_parameters((VOCAB_SIZE, HIDDEN_DIM), name=\"hidden2out\")\n",
    "        self.bias = model.add_parameters((VOCAB_SIZE), name=\"bias\")\n",
    "\n",
    "    def save_to_disk(self, filename):\n",
    "        dy.save(filename, [self.builder, self.lookup, self.R, self.bias])\n",
    "\n",
    "    def load_from_disk(self, filename):\n",
    "        (self.builder, self.lookup, self.R, self.bias) = dy.load(filename, model)\n",
    "        \n",
    "    def build_lm_graph(self, sent):\n",
    "        dy.renew_cg()\n",
    "        init_state = self.builder.initial_state()\n",
    "\n",
    "        errs = [] # will hold expressions\n",
    "        es=[]\n",
    "        state = init_state\n",
    "        for (cw,nw) in zip(sent,sent[1:]):\n",
    "            # assume word is already a word-id\n",
    "            x_t = dy.lookup(self.lookup, int(cw))\n",
    "            state = state.add_input(x_t)\n",
    "            y_t = state.output()\n",
    "            r_t = self.bias + (self.R * y_t)\n",
    "            err = dy.pickneglogsoftmax(r_t, int(nw))\n",
    "            errs.append(err)\n",
    "        nerr = dy.esum(errs)\n",
    "        return nerr\n",
    "    \n",
    "    def predict_next_word(self, sentence):\n",
    "        dy.renew_cg()\n",
    "        init_state = self.builder.initial_state()\n",
    "        state = init_state\n",
    "        for cw in sentence:\n",
    "            # assume word is already a word-id\n",
    "            x_t = self.lookup[int(cw)]\n",
    "            state = state.add_input(x_t)\n",
    "        y_t = state.output()\n",
    "        r_t = self.bias + (self.R * y_t)\n",
    "        prob = dy.softmax(r_t)\n",
    "        return prob\n",
    "        \n",
    "    def sample(self, first=1, nchars=0, stop=-1):\n",
    "        res = [first]\n",
    "        dy.renew_cg()\n",
    "        state = self.builder.initial_state()\n",
    "\n",
    "        cw = first\n",
    "        while True:\n",
    "            x_t = self.lookup[cw]\n",
    "            state = state.add_input(x_t)\n",
    "            y_t = state.output()\n",
    "            r_t = self.bias + (self.R * y_t)\n",
    "            ydist = dy.softmax(r_t)\n",
    "            dist = ydist.vec_value()\n",
    "            rnd = random.random()\n",
    "            for i,p in enumerate(dist):\n",
    "                rnd -= p\n",
    "                if rnd <= 0: break\n",
    "            res.append(i)\n",
    "            cw = i\n",
    "            if cw == stop: break\n",
    "            if nchars and len(res) > nchars: break\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24158\n",
      "<s>żzpVpúsãúIvżf ó\n",
      "2500 24158\n",
      "3.312249622638672\n",
      "<s>Agüaer\n",
      "5000 24158\n",
      "3.3096209765548243\n",
      "<s>Collin\n",
      "7500 24158\n",
      "3.315030470705376\n",
      "<s>Sode\n",
      "10000 24158\n",
      "3.3302472654334774\n",
      "<s>Araikkeskayy\n",
      "12500 24158\n",
      "3.341012694952305\n",
      "<s>Charo\n",
      "15000 24158\n",
      "3.3434368001072747\n",
      "<s>Elusi\n",
      "17500 24158\n",
      "3.341647548855079\n",
      "<s>Cooper\n",
      "20000 24158\n",
      "3.3617703706721214\n",
      "<s>At\n",
      "22500 24158\n",
      "3.3823963349407498\n",
      "<s>Dejchenef\n",
      "TM: 6.840626398722331e-05\n",
      "ITER 0, loss=44670.92741680145\n",
      "0 24158\n",
      "3.3324078639911563\n",
      "<s>Balanov\n",
      "2500 24158\n",
      "3.363216893229123\n",
      "<s>Adniani\n",
      "5000 24158\n",
      "3.371980296670738\n",
      "<s>Sasgol\n",
      "7500 24158\n",
      "3.3632297635702564\n",
      "<s>Vanara\n",
      "10000 24158\n",
      "3.3505029714091425\n",
      "<s>Jilyakov\n",
      "12500 24158\n",
      "3.361541686939542\n",
      "<s>Gurin\n",
      "15000 24158\n",
      "3.3866671085333886\n",
      "<s>Chena\n",
      "17500 24158\n",
      "3.362624272731542\n",
      "<s>Eosion\n",
      "20000 24158\n",
      "3.3636006569254717\n",
      "<s>Mis\n",
      "22500 24158\n",
      "3.3638998851674833\n",
      "<s>Jonsi\n",
      "TM: 6.514787673950195e-05\n",
      "ITER 1, loss=45200.18455505371\n",
      "0 24158\n",
      "3.371889933237875\n",
      "<s>Sapinh\n",
      "2500 24158\n",
      "3.365280220255356\n",
      "<s>Roriechietu\n",
      "5000 24158\n",
      "3.359764774769603\n",
      "<s>Isas\n",
      "7500 24158\n",
      "3.370783594586014\n",
      "<s>Tauseno\n",
      "10000 24158\n",
      "3.382207947423306\n",
      "<s>Juling\n",
      "12500 24158\n",
      "3.341873290827456\n",
      "<s>Arile\n",
      "15000 24158\n",
      "3.371815380475701\n",
      "<s>Manyanbau\n",
      "17500 24158\n",
      "3.3600352293024556\n",
      "<s>Janan\n",
      "20000 24158\n",
      "3.3560370296574376\n",
      "<s>Vofiselfiter\n",
      "22500 24158\n",
      "3.3673505481039108\n",
      "<s>Hva\n",
      "TM: 6.505421229771205e-05\n",
      "ITER 2, loss=44562.630986213684\n",
      "loading the saved model...\n",
      "<s>Elias\n"
     ]
    }
   ],
   "source": [
    "corpus = \"allnames.txt\"\n",
    "\n",
    "LAYERS = 2\n",
    "INPUT_DIM = 32 #50  #256\n",
    "HIDDEN_DIM = 128 # 50  #1024\n",
    "\n",
    "train = util.CharsCorpusReader(corpus, begin=\"<s>\")\n",
    "vocab = util.Vocab.from_corpus(train)\n",
    "\n",
    "VOCAB_SIZE = vocab.size()\n",
    "\n",
    "model = dy.Model()\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.2)\n",
    "\n",
    "lm = RNNLanguageModel(model, LAYERS, INPUT_DIM, HIDDEN_DIM, VOCAB_SIZE, builder=dy.SimpleRNNBuilder)\n",
    "#lm = RNNLanguageModel(model, LAYERS, INPUT_DIM, HIDDEN_DIM, VOCAB_SIZE, builder=dy.LSTMBuilder)\n",
    "\n",
    "\n",
    "train = list(train)\n",
    "\n",
    "losses = []\n",
    "\n",
    "chars = loss = 0.0\n",
    "\n",
    "for ITER in range(3):\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    for i,sent in enumerate(train):\n",
    "        _start = time.time()\n",
    "        \n",
    "        if i % 2500 == 0:\n",
    "            trainer.status()\n",
    "            print (i,len(train))\n",
    "            \n",
    "            if chars > 0: print(loss / chars,)\n",
    "                \n",
    "            for _ in range(1):\n",
    "                samp = lm.sample(first=vocab.w2i[\"<s>\"],stop=vocab.w2i[\"\\n\"])\n",
    "                print(\"\".join([vocab.i2w[c] for c in samp]).strip())\n",
    "            loss = 0.0\n",
    "            chars = 0.0\n",
    "\n",
    "        chars += len(sent)-1\n",
    "        isent = [vocab.w2i[w] for w in sent]\n",
    "        errs = lm.build_lm_graph(isent)\n",
    "        loss += errs.scalar_value()\n",
    "        errs.backward()\n",
    "        trainer.update()\n",
    "    print (\"TM:\",(time.time() - _start)/len(sent))\n",
    "    print(\"ITER {}, loss={}\".format(ITER, loss))\n",
    "    losses.append(loss)\n",
    "    trainer.status()\n",
    "\n",
    "lm.save_to_disk(\"RNNLanguageModel.model\")\n",
    "\n",
    "print(\"loading the saved model...\")\n",
    "lm.load_from_disk(\"RNNLanguageModel.model\")\n",
    "samp = lm.sample(first=vocab.w2i[\"<s>\"],stop=vocab.w2i[\"\\n\"])\n",
    "print(\"\".join([vocab.i2w[c] for c in samp]).strip())\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

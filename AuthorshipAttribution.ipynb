{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T10:55:37.839006Z",
     "start_time": "2019-05-01T10:55:14.948157Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import dynet as dy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path: str, label: str):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(path+os.sep+label+os.sep+'*.txt')\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label))\n",
    "        f.close()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_text(text, n: int):\n",
    "    \"\"\"\n",
    "    Extracts all character 'n'-grams from a given 'text'.\n",
    "    Each digit is represented as a hashtag symbol (#) which in general denotes any number.\n",
    "    Each hyperlink is replaced by an @ sign.\n",
    "    The latter steps are computed through regular expressions.\n",
    "    \"\"\"    \n",
    "    if n > 0:\n",
    "        text = re.sub(\"[0-9]+(([.,^])[0-9]+)?\", \"#\", text)\n",
    "        text = re.sub(\"https:\\\\\\+([a-zA-Z0-9.]+)?\", \"@\", text)\n",
    "        tokens = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    \n",
    "    # create frequency text representation (keys are tokens, values are their corresponding frequencies)\n",
    "    frequency = {token: tokens.count(token) for token in list(set(tokens))}\n",
    "        \n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(texts, n: int, ft = 3):\n",
    "    \n",
    "    # Extracts all characer 'n'-grams occurring at least 'ft' times in a set of 'texts'\n",
    "    occurrences=defaultdict(int)\n",
    "    \n",
    "    for (text,label) in texts:\n",
    "        \n",
    "        text_occurrences=represent_text(text,n)\n",
    "        \n",
    "        for ngram in text_occurrences.keys():\n",
    "            \n",
    "            if ngram in occurrences:\n",
    "                occurrences[ngram] += text_occurrences[ngram]\n",
    "            else:\n",
    "                occurrences[ngram] = text_occurrences[ngram]\n",
    "    \n",
    "    vocabulary=[]\n",
    "    for i in occurrences.keys():\n",
    "        if occurrences[i] >= ft:\n",
    "            vocabulary.append(i)\n",
    "            \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(word, documents):\n",
    "    n_documents = 0\n",
    "    for document in documents:\n",
    "        if document.count(word) > 0:\n",
    "            n_documents += 1\n",
    "    return n_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(documents, vocabulary):\n",
    "    D = len(documents)\n",
    "    weights_matrix = []\n",
    "    for document in documents:\n",
    "        unique_words_per_doc = set(document)\n",
    "        one_hot_tf_idf = np.zeros(len(vocabulary))\n",
    "        for word in unique_words_per_doc:\n",
    "            for i, w in enumerate(vocabulary):\n",
    "                if w == word:\n",
    "                    tf = document.count(word) / len(document)\n",
    "                    # if the term is not in the corpus, this will lead to a division-by-zero\n",
    "                    # therefore, in the denominator we add the idf to 1\n",
    "                    idf = np.log((D / (1 + get_idf(word, documents))))\n",
    "                    one_hot_tf_idf[i] += tf * idf\n",
    "                    break\n",
    "        weights_matrix.append(one_hot_tf_idf)\n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(path,outpath,n_range=3,pt=0.1, lower = False):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    language = []\n",
    "    \n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "            language.append(attrib['language'])\n",
    "            \n",
    "    for index, problem in enumerate(problems):\n",
    "        print(problem)\n",
    "        # Reading information about the problem\n",
    "        infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "        candidates = []\n",
    "        with open(infoproblem, 'r') as f:\n",
    "            fj = json.load(f)\n",
    "            unk_folder = fj['unknown-folder']\n",
    "            for attrib in fj['candidate-authors']:\n",
    "                candidates.append(attrib['author-name'])\n",
    "                \n",
    "        # Building training set\n",
    "        train_docs = []\n",
    "        for candidate in candidates:\n",
    "            train_docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "        train_texts = [text for i,(text,label) in enumerate(train_docs)]\n",
    "        train_labels = [label for i,(text,label) in enumerate(train_docs)]\n",
    "        \n",
    "        vocab = []\n",
    "        for n in range(2, n_range + 1):\n",
    "            n_vocab = extract_vocabulary(train_docs, n, (n_range - n) + 1)\n",
    "            vocab.extend(n_vocab)\n",
    "        \n",
    "        vectorizer = CountVectorizer(analyzer='char', ngram_range = (2, n_range), lowercase = lower, vocabulary = vocab)\n",
    "        train_data = vectorizer.fit_transform(train_texts)\n",
    "        train_data = train_data.toarray()\n",
    "        train_data_pca = pca.fit(train_data)\n",
    "        \n",
    "        for i,v in enumerate(train_texts):\n",
    "            train_data[i]=train_data[i]/len(train_texts[i])\n",
    "            \n",
    "        print('\\t', 'language: ', language[index])\n",
    "        print('\\t', len(candidates), 'candidate authors')\n",
    "        print('\\t', len(train_texts), 'known texts')\n",
    "        print('\\t', 'vocabulary size:', len(vocabulary))\n",
    "        \n",
    "        # Building test set\n",
    "        test_docs = read_files(path+os.sep+problem,unk_folder)\n",
    "        test_texts = [text for i,(text,label) in enumerate(test_docs)]\n",
    "        test_data = vectorizer.transform(test_texts)\n",
    "        test_data = test_data.toarray()\n",
    "        test_data_pca = pca.transform(n_components)\n",
    "        \n",
    "        for i,v in enumerate(test_texts):\n",
    "            test_data[i]=test_data[i]/len(test_texts[i])\n",
    "        print('\\t', len(test_texts), 'unknown texts')\n",
    "        \n",
    "        # Applying SVM\n",
    "        max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "        scaled_train_data = max_abs_scaler.fit_transform(train_data)\n",
    "        scaled_test_data = max_abs_scaler.transform(test_data)\n",
    "        clf=CalibratedClassifierCV(OneVsRestClassifier(SVC(C=1)))\n",
    "        clf.fit(scaled_train_data, train_labels)\n",
    "        predictions=clf.predict(scaled_test_data)\n",
    "        proba=clf.predict_proba(scaled_test_data)\n",
    "        \n",
    "        # Reject option (used in open-set cases)\n",
    "        count=0\n",
    "        for i,p in enumerate(predictions):\n",
    "            sproba=sorted(proba[i],reverse=True)\n",
    "            if sproba[0]-sproba[1]<pt:\n",
    "                predictions[i]=u'<UNK>'\n",
    "                count=count+1\n",
    "        print('\\t',count,'texts left unattributed')\n",
    "        # Saving output data\n",
    "        out_data=[]\n",
    "        unk_filelist = glob.glob(path+os.sep+problem+os.sep+unk_folder+os.sep+'*.txt')\n",
    "        pathlen=len(path+os.sep+problem+os.sep+unk_folder+os.sep)\n",
    "        for i,v in enumerate(predictions):\n",
    "            out_data.append({'unknown-text': unk_filelist[i][pathlen:], 'predicted-author': v})\n",
    "        with open(outpath+os.sep+'answers-'+problem+'.json', 'w') as f:\n",
    "            json.dump(out_data, f, indent=4)\n",
    "        print('\\t', 'answers saved to file','answers-'+problem+'.json')\n",
    "    print('elapsed time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T11:32:01.464425Z",
     "start_time": "2019-04-28T11:32:01.459412Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_cleaning(document: list):\n",
    "    \"\"\"\n",
    "    Cleans each document in the corpus accordingly.\n",
    "    Converts each name to lower-case and only keeps strings,\n",
    "    which contain alphabetical characters and have more than 1 character (similar to sklearn's CountVectorizer).\n",
    "    Function filters for non-alpha characters (e.g., digits, special characters, punctuation marks).\n",
    "    \"\"\"\n",
    "    cleaned_names = list()\n",
    "    for name in document:\n",
    "        \n",
    "        if len(name) > 1:\n",
    "            cleaned_name = ''.join([char for char in name if char.isalpha()]).lower()\n",
    "            \n",
    "            if len(cleaned_name) > 1:\n",
    "                cleaned_names.append(cleaned_name)  \n",
    "\n",
    "    return cleaned_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T11:32:02.372298Z",
     "start_time": "2019-04-28T11:32:02.366283Z"
    }
   },
   "outputs": [],
   "source": [
    "# this function is only used for personal purposes but not for actual computation\n",
    "\n",
    "def extract_vocabulary(names: list, n = 1):\n",
    "    \"\"\"\n",
    "    Create vocabulary of character bigrams.\n",
    "    Include bigrams, if and only if characters are alphabetical.\n",
    "    Discard non-alphabetical characters (i.e., no bigrams will be created for non-alphabetical characters (e.g., digits, special characters)).\n",
    "    \"\"\"\n",
    "    bigrams = []\n",
    "    \n",
    "    for name in names:\n",
    "        for i, char in enumerate(name):\n",
    "            if i > 0:\n",
    "                # create character bigram, if and only if characters are alphabetical\n",
    "                if name[i].isalpha() and name[i-n].isalpha():\n",
    "                    bigram = name[i] + name[i-n] \n",
    "                    bigrams.append(bigram)\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "    # vocabulary is a dictionary, where keys are unique bigrams and values are their corresponding frequency in the corpus\n",
    "    vocab = {bigram: bigrams.count(bigram) for bigram in sorted(list(set(bigrams)))}\n",
    "    return vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T11:32:04.466973Z",
     "start_time": "2019-04-28T11:32:04.237590Z"
    }
   },
   "outputs": [],
   "source": [
    "all_names = names_english + names_german\n",
    "vocab = extract_vocabulary(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:26:22.502194Z",
     "start_time": "2019-04-27T15:26:22.496178Z"
    }
   },
   "outputs": [],
   "source": [
    "def bag_of_chars(names_english: list, names_german: list, vocab = None):\n",
    "    \"\"\"\n",
    "    Create a bag-of-character bigram representation of provided names.\n",
    "    Vocabulary argument is set to None by default. \n",
    "    You can pass a vocabulary to this function, which will then be used for CountVectorizer. \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize vectorizer (character-bigram representation)\n",
    "    vectorizer = CountVectorizer(lowercase = True, ngram_range = (2, 2), analyzer = 'char')\n",
    "    \n",
    "    # represent names as bag-of-character bigrams\n",
    "    all_names = text_cleaning(names_english) + text_cleaning(names_german)\n",
    "    \n",
    "    # create train set\n",
    "    X_train = vectorizer.fit_transform(all_names).toarray()\n",
    "    \n",
    "    # create labels (0 for english names, 1 for german names)\n",
    "    labels_eng = np.zeros(len(names_english), dtype = int) #[0 for name in names_english]\n",
    "    labels_ger = np.ones(len(names_german), dtype = int) #[1 for name in names_german]\n",
    "    \n",
    "    # concatenate labels\n",
    "    y_train = np.hstack((labels_eng, labels_ger))\n",
    "    \n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "    \n",
    "    vocab = sorted([bigram for bigram in vectorizer.vocabulary_.keys()])\n",
    "    \n",
    "    return X_train, y_train, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:26:24.231757Z",
     "start_time": "2019-04-27T15:26:24.160305Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, vocab = bag_of_chars(names_english, names_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:26:24.456509Z",
     "start_time": "2019-04-27T15:26:24.440467Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:52:38.031396Z",
     "start_time": "2019-04-27T15:51:40.648883Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5918369834024615\n",
      "0.34798219804066777\n",
      "0.32447194521110134\n",
      "0.3012735612541166\n",
      "0.276217661770858\n",
      "0.25976176490712033\n",
      "0.24850757459419456\n",
      "0.23962345723408818\n",
      "0.23214402675411522\n",
      "0.22564043971847314\n",
      "0.21987740566550273\n",
      "0.2147053768535998\n",
      "0.21001896207884144\n",
      "0.20574577014302947\n",
      "0.2018376421895835\n",
      "0.1982575746181884\n",
      "0.19497210976204585\n",
      "0.19195098531702176\n",
      "0.1891682494180667\n",
      "0.18660182296849773\n",
      "0.18423224096686897\n",
      "0.18204123883733767\n",
      "0.18001100925799926\n",
      "0.1781242705325177\n",
      "0.1763647956084901\n",
      "0.17471799460699433\n",
      "0.17317115991819101\n",
      "0.17171330053786762\n",
      "0.17033486912376458\n",
      "0.16902741329754636\n",
      "0.16778337724764708\n",
      "0.16659611458118279\n",
      "0.16545987663916117\n",
      "0.16436977608499198\n",
      "0.16332173700323957\n",
      "0.16231240868188426\n",
      "0.16133906601339523\n",
      "0.16039948294342976\n",
      "0.15949174962625695\n",
      "0.1586141287466216\n",
      "0.15776486070538262\n",
      "0.15694211496681462\n",
      "0.15614397029896251\n",
      "0.1553685539054306\n",
      "0.15461414327712658\n",
      "0.1538793571795488\n",
      "0.1531632087025486\n",
      "0.1524650583854573\n",
      "0.151784551301072\n",
      "0.15112153938776374\n",
      "0.15047608215517902\n",
      "0.1498483884502847\n",
      "0.14923877244601486\n",
      "0.1486474619075683\n",
      "0.148074472889657\n",
      "0.14751950655478596\n",
      "0.14698197718417927\n",
      "0.14646106468469502\n",
      "0.14595586475398806\n",
      "0.14546544262240277\n",
      "0.14498887088943702\n",
      "0.14452526263743368\n",
      "0.14407379584614263\n",
      "0.1436336597458261\n",
      "0.14320412550956174\n",
      "0.14278446737751502\n",
      "0.1423740101425157\n",
      "0.1419721337506897\n",
      "0.14157821561035128\n",
      "0.14119169683295738\n",
      "0.1408120314633043\n",
      "0.14043872012249542\n",
      "0.14007129282682101\n",
      "0.13970933200448807\n",
      "0.13935247522342833\n",
      "0.13900038973175546\n",
      "0.13865284576930634\n",
      "0.13830963596837334\n",
      "0.13797066254856807\n",
      "0.13763587167299945\n",
      "0.13730525726177653\n",
      "0.13697886758760458\n",
      "0.1366567555373918\n",
      "0.13633900593104475\n",
      "0.1360257324408095\n",
      "0.13571705234886736\n",
      "0.13541309287150702\n",
      "0.13511398028860544\n",
      "0.13481985119414025\n",
      "0.13453082384741805\n",
      "0.1342469543110043\n",
      "0.13396827862996655\n",
      "0.13369478326026207\n",
      "0.1334264062222887\n",
      "0.1331630467771398\n",
      "0.13290456712300225\n",
      "0.13265083213158643\n",
      "0.13240170573819096\n",
      "0.13215699870621136\n",
      "0.1319165659732506\n"
     ]
    }
   ],
   "source": [
    "#MODEL SPECIFICATION\n",
    "model = dy.ParameterCollection()\n",
    "\n",
    "W_p = model.add_parameters((2, X_train.shape[1]))\n",
    "b_p = model.add_parameters(2)\n",
    "\n",
    "#trainer = dy.SimpleSGDTrainer(model)\n",
    "trainer = dy.AdamTrainer(model) # adam is more efficient than stochastic gradient descent\n",
    "\n",
    "num_epochs = 100\n",
    "tolerance = 1e-6\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    cur_loss = 0\n",
    "\n",
    "    for input_x, out_label in zip(X_train,y_train):\n",
    "        \n",
    "        dy.renew_cg() # create a new computation graph for each training example\n",
    "        \n",
    "        x = dy.inputVector(input_x) # this must go after renewing the graph\n",
    "        \n",
    "        # sigmoid activation function (for binary classification tasks)\n",
    "        output_value = dy.logistic(W_p * x + b_p)\n",
    "        \n",
    "        # forward propagation (i.e., compute loss)\n",
    "        loss = dy.hinge(output_value, out_label)\n",
    "\n",
    "        loss_val = loss.value()\n",
    "        cur_loss = cur_loss + loss_val\n",
    "        \n",
    "        # backward propagation (i.e., compute gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        trainer.update()\n",
    "        \n",
    "    \n",
    "    print(cur_loss/len(y_train))\n",
    "    losses.append(cur_loss/len(y_train))\n",
    "    \n",
    "    if epoch > 0:\n",
    "        diff = abs(losses[epoch - 1] - cur_loss/len(y_train))\n",
    "        \n",
    "        # if losses do not change significantly any longer, gradient descent has converged (break loop)\n",
    "        if diff < tolerance: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:30:54.838478Z",
     "start_time": "2019-04-27T15:30:54.677678Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4XPV97/H3RyONdlu2JRss23jBQAzYOAiThjRLC4SkBMgObZql6eXmJiS5SXpvyb29aUtueilp02w0gSSQtE1CSOji8JAQQiE7YBF2g8E2i4WxLa+yrV363j/mSB7LI0tejkfWfF7PM4/mbDPf45Hno3N+5/x+igjMzMwOpqzYBZiZ2cTnsDAzszE5LMzMbEwOCzMzG5PDwszMxuSwMDOzMTkszMxsTA4LMzMbk8PCzMzGVJ7mi0u6CPgCkAG+HhHXjlj+XuCzwIvJrC9HxNeTZe8B/iKZ/38j4lsHe6/GxsaYP3/+0SvezKwEPPjgg1sjomms9VILC0kZ4HrgAqANWCVpZUSsHrHq9yLiqhHbTgf+EmgBAngw2XbHaO83f/58Wltbj+o+mJlNdpKeH896aZ6GWgGsjYj1EdEL3AJcOs5tXw/cFRHbk4C4C7gopTrNzGwMaYZFM7Ahb7otmTfSWyU9KukHkuYe4rZmZnYMpBkWKjBvZBe3PwTmR8RS4KfAULvEeLZF0pWSWiW1tre3H1GxZmY2ujTDog2Ymzc9B9iYv0JEbIuInmTya8DZ49022f7GiGiJiJampjHbZ8zM7DClGRargMWSFkjKApcDK/NXkHRi3uQlwJPJ8zuBCyVNkzQNuDCZZ2ZmRZDa1VAR0S/pKnJf8hngpoh4QtI1QGtErAQ+IukSoB/YDrw32Xa7pE+TCxyAayJie1q1mpnZwWmyjJTX0tISvnTWzOzQSHowIlrGWq/k7+De09PP5+56modeGPUWDjOzklfyYdHXP8gX736GhzfsLHYpZmYTVsmHRVVFBoDuvsEiV2JmNnGVfFhUluf+Cbr6BopciZnZxFXyYVFWJqoqyuh2WJiZjarkwwJyp6IcFmZmo3NYANUVGbp6HRZmZqNxWJALi+5+N3CbmY3GYQFU+sjCzOygHBZAtRu4zcwOymGBG7jNzMbisCBp4HZYmJmNymEBVGUdFmZmB+OwAKrKM/S4uw8zs1E5LIDqbJmPLMzMDsJhgW/KMzMbi8OC5Gqo/gEmy0BQZmZHm8OCXFhEQI/v4jYzK8hhQe40FOBGbjOzUaQaFpIukrRG0lpJVx9kvbdJCkktyfR8SV2SHk4eX02zzqEBkNzIbWZWWHlaLywpA1wPXAC0AaskrYyI1SPWqwc+Atw/4iXWRcRZadWXrzrrAZDMzA4mzSOLFcDaiFgfEb3ALcClBdb7NHAd0J1iLQdVVT40tKrDwsyskDTDohnYkDfdlswbJmk5MDcibi+w/QJJD0n6maTfLfQGkq6U1Cqptb29/bALrcr6NJSZ2cGkGRYqMG/42lRJZcA/AJ8osN5LwLyIWA58HPiOpCkHvFjEjRHREhEtTU1Nh13oUAN3t++1MDMrKM2waAPm5k3PATbmTdcDZwD3SnoOeAWwUlJLRPRExDaAiHgQWAecklahQw3c3f0OCzOzQtIMi1XAYkkLJGWBy4GVQwsjYldENEbE/IiYD9wHXBIRrZKakgZyJC0EFgPr0yp06Miiq9eXzpqZFZLa1VAR0S/pKuBOIAPcFBFPSLoGaI2IlQfZ/NXANZL6gQHgAxGxPa1aq33prJnZQaUWFgARcQdwx4h5nxpl3dfmPb8NuC3N2vJVVeQOsHw1lJlZYb6Dm31XQzkszMwKc1iQdzWUw8LMrCCHBVCRKSNTJrdZmJmNwmGRyI1p4auhzMwKcVgkqirKfJ+FmdkoHBaJqoqM7+A2MxuFwyJRXZFxm4WZ2SgcFomqioyvhjIzG4XDIuEjCzOz0TksElXZDF0eVtXMrCCHRaKqvIweH1mYmRXksEhUZ30aysxsNA6LRO6mPIeFmVkhDouEr4YyMxudwyKRCws3cJuZFeKwSFRVlNE7MMjAYIy9splZiXFYJNxNuZnZ6BwWieqsh1Y1MxuNwyJRVe4jCzOz0aQaFpIukrRG0lpJVx9kvbdJCkktefM+mWy3RtLr06wTPLSqmdnBlKf1wpIywPXABUAbsErSyohYPWK9euAjwP1585YAlwOnA7OBn0o6JSJS+yYfarPwAEhmZgdK88hiBbA2ItZHRC9wC3BpgfU+DVwHdOfNuxS4JSJ6IuJZYG3yeqmpqsj9U3gAJDOzA6UZFs3AhrzptmTeMEnLgbkRcfuhbptsf6WkVkmt7e3tR1TsviMLh4WZ2UhphoUKzBu+iUFSGfAPwCcOddvhGRE3RkRLRLQ0NTUddqGQuykPfDWUmVkhqbVZkDsamJs3PQfYmDddD5wB3CsJ4ARgpaRLxrHtUVfl+yzMzEaV5pHFKmCxpAWSsuQarFcOLYyIXRHRGBHzI2I+cB9wSUS0JutdLqlS0gJgMfBAirUO32fhsDAzO1BqRxYR0S/pKuBOIAPcFBFPSLoGaI2IlQfZ9glJtwKrgX7gQ2leCQW58SwA9w9lZlZAmqehiIg7gDtGzPvUKOu+dsT0Z4DPpFbcCL6D28xsdL6DOzF0B7evhjIzO5DDIlFWJrLlZb7PwsysAIdFnuqKDN0+sjAzO4DDIk91hcfhNjMrxGGRp6qizFdDmZkV4LDIU+UjCzOzghwWeaqzGd+UZ2ZWgMMiT1W5w8LMrBCHRZ7qrE9DmZkV4rDI4wZuM7PCHBZ5qioyvoPbzKwAh0We6gq3WZiZFeKwyFPlsDAzK8hhkWfoDu6IAwblMzMraQ6LPNXZDIMBvQNu5DYzy+ewyFPpAZDMzApyWOTx0KpmZoU5LPJUV3gAJDOzQlINC0kXSVojaa2kqwss/4CkxyQ9LOmXkpYk8+dL6krmPyzpq2nWOaQqCQsPgGRmtr/UxuCWlAGuBy4A2oBVklZGxOq81b4TEV9N1r8E+BxwUbJsXUSclVZ9hfjIwsyssDSPLFYAayNifUT0ArcAl+avEBEdeZO1QFGvWa2scAO3mVkhaYZFM7Ahb7otmbcfSR+StA64DvhI3qIFkh6S9DNJv5tincOGjizcwG1mtr80w0IF5h1w5BAR10fEIuDPgb9IZr8EzIuI5cDHge9ImnLAG0hXSmqV1Nre3n7EBQ9dDeWeZ83M9pdmWLQBc/Om5wAbD7L+LcBlABHRExHbkucPAuuAU0ZuEBE3RkRLRLQ0NTUdccFV5T6yMDMrJM2wWAUslrRAUha4HFiZv4KkxXmTfwA8k8xvShrIkbQQWAysT7FWwEcWZmajSe1qqIjol3QVcCeQAW6KiCckXQO0RsRK4CpJ5wN9wA7gPcnmrwaukdQPDAAfiIjtadU6pMpXQ5mZFZRaWABExB3AHSPmfSrv+UdH2e424LY0ayukKrkaqtNhYWa2H9/BnaeyPMOiplp+vW5rsUsxM5tQHBYjvGnZbO5/djubdnUXuxQzswnDYTHCJctmEwG3P3qwC7fMzEqLw2KEhU11nNE8hR8+4rAwMxvisCjgTUtn80jbLp7burfYpZiZTQgOiwIuXjYb8KkoM7MhDosCmhuqOWf+NFb6VJSZGeCwGNWbls3m6c17eGpTx9grm5lNcuMKC0mLJFUmz18r6SOSGtItrbjeeOaJZMrE7Y+8VOxSzMyKbrxHFrcBA5JOBr4BLAC+k1pVE0BjXSXL5zbwi2eOvDdbM7Pj3XjDYjAi+oE3A5+PiI8BJ6ZX1sRw3smNPPriLnZ19hW7FDOzohpvWPRJuoJcR3+3J/Mq0ilp4njV4kYi4DfrtxW7FDOzohpvWLwP+B3gMxHxrKQFwL+kV9bEsGxOAzXZDL9a676izKy0javX2YhYTTLkqaRpQH1EXJtmYRNBtryMcxdMd1iYWckb79VQ90qaImk68Ahws6TPpVvaxHDeyY2s37qXjTu7il2KmVnRjPc01NSI6ADeAtwcEWcD56dX1sTxqsWNAD66MLOSNt6wKJd0IvAO9jVwl4RTZ9XTWJd1WJhZSRtvWFxDbnjUdRGxKhkX+5n0ypo4JPHKRY38cu02IqLY5ZiZFcW4wiIivh8RSyPivyXT6yPiremWNnG86uRGtu7p4enNe4pdiplZUYy3gXuOpH+TtEXSZkm3SZozju0ukrRG0lpJVxdY/gFJj0l6WNIvJS3JW/bJZLs1kl5/aLt1dJ3ndgszK3HjPQ11M7ASmA00Az9M5o1KUga4HngDsAS4Ij8MEt+JiDMj4izgOuBzybZLgMuB04GLgH9MXq8omhuqOWlGDff55jwzK1HjDYumiLg5IvqTxzeBpjG2WQGsTU5Z9QK3AJfmr5BcYTWkFhhqFLgUuCUieiLiWWBt8npFc9bcBh5t21XMEszMima8YbFV0rskZZLHu4Cx/sxuBjbkTbcl8/Yj6UOS1pE7svjIoWx7LC2d08Cmjm62dHQXswwzs6IYb1j8CbnLZjcBLwFvI9cFyMGowLwDLieKiOsjYhHw58BfHMq2kq6U1Cqptb093d5hl82ZCsAjProwsxI03quhXoiISyKiKSJmRsRl5G7QO5g2YG7e9BzgYEPP3QJcdijbRsSNEdESES1NTWOdFTsyp8+eSqZMPNq2M9X3MTObiI5kpLyPj7F8FbBY0gJJWXIN1ivzV5C0OG/yD9h378ZK4HJJlUmnhYuBB46g1iNWnc2weGadjyzMrCSNqyPBURQ6VTQsIvolXUXuZr4McFNEPCHpGqA1IlYCV0k6H+gDdpDrAp1kvVuB1UA/8KGIGDiCWo+KZXMauHP1JiIC6aC7b2Y2qRxJWIx5O3NE3AHcMWLep/Kef/Qg234G+MwR1HfULZ07le+1bmDD9i7mzagpdjlmZsfMQcNC0m4Kh4KA6lQqmsCWzckNO/5I206HhZmVlIO2WUREfURMKfCoj4gjOSo5Lp16Qj3Z8jI3cptZyTmSBu6SU5EpY8mJU9zIbWYlx2FxiJbNmcrjL+5iYNA90JpZ6XBYHKKlcxro7B1gXbt7oDWz0uGwOETL5iZ3cm9wu4WZlQ6HxSFa2FhHXWW5OxU0s5LisDhEZWVi+bwGfrXOY1uYWelwWByGC5bMYn37XtZucbuFmZUGh8VhOP9lswC4a/XmIldiZnZsOCwOw+yGas5snspPVm8qdilmZseEw+IwXbhkFg9v2OnBkMysJDgsDtOFp59ABPz0yS3FLsXMLHUOi8N0yqw6TppR41NRZlYSHBaHSRIXLpnFr9duY3d3X7HLMTNLlcPiCFx4+gn0Dgzys6fTHf/bzKzYHBZH4OXzpjGjNsuPH/epKDOb3BwWRyBTJt60bDZ3PrGJjTu7il2OmVlqHBZH6L+8eiERcMPP1hW7FDOz1KQaFpIukrRG0lpJVxdY/nFJqyU9KuluSSflLRuQ9HDyWJlmnUeiuaGat758Dt9dtcH3XJjZpJVaWEjKANcDbwCWAFdIWjJitYeAlohYCvwAuC5vWVdEnJU8LkmrzqPhg69bxMBg8LVfrC92KWZmqUjzyGIFsDYi1kdEL3ALcGn+ChFxT0R0JpP3AXNSrCc1J82o5dJls/mX+15g256eYpdjZnbUpRkWzcCGvOm2ZN5o3g/8KG+6SlKrpPskXVZoA0lXJuu0trcX9/LVD77uZLr7B/jGL58tah1mZmlIMyxUYF7BgaslvQtoAT6bN3teRLQAfwh8XtKiA14s4saIaImIlqampqNR82E7eWYdFy+dzc2/eo4N2zvH3sDM7DiSZli0AXPzpucAG0euJOl84H8Dl0TE8DmciNiY/FwP3AssT7HWo+KTbziNMsH/+Y/HiSiYi2Zmx6U0w2IVsFjSAklZ4HJgv6uaJC0HbiAXFFvy5k+TVJk8bwTOA1anWOtRMbuhmk9ceCr3rmnn9kdfKnY5ZmZHTWphERH9wFXAncCTwK0R8YSkayQNXd30WaAO+P6IS2RfBrRKegS4B7g2IiZ8WAC855XzObN5Kn/9w9Xs6nSfUWY2OWiynC5paWmJ1tbWYpcBwOMv7uKSL/+Sd54zl//3lqXFLsfMbFSSHkzahw/Kd3Cn4Izmqfzp7y7kuw9s4M4n3G+UmR3/HBYp+cSFp3BG8xT+x/cfoW2Hr44ys+ObwyIlleUZvnzFyxkM+PB3H6JvYLDYJZmZHTaHRYrmN9Zy7VvP5KEXdvJ3d64pdjlmZofNYZGyi5fO5o/OnccNP1/Pfzz8YrHLMTM7LA6LY+BTb1rCigXT+R/ff5TW57YXuxwzs0PmsDgGKssz3PCus5ndUMWV//wgL2xzg7eZHV8cFsfItNosN733HAYGg/d98wG27+0tdklmZuPmsDiGFjbVceMfn03bji7efdP97OryHd5mdnxwWBxj5y6cwVffdTZrNu3mfTc/wN6e/mKXZGY2JodFEbzutJl88fLlPLxhJ+//1io6ex0YZjaxOSyK5A1nnsjn3nEWDzy7nXd/4wGfkjKzCc1hUUSXLW/mS1e8nEfadnLFjfex1UOymtkE5bAosj9YeiJfe3cL67fu4R03/Maj7JnZhOSwmABee+pM/ulPzmXr7h7e/I+/4rcv7Ch2SWZm+3FYTBArFkznXz94HjXZci6/8T5++MgBI9CamRWNw2ICOXlmHf/+ofNY2jyVD3/3If7uzjUMDE6OwanM7PjmsJhgptdm+fZ/OZd3tMzhy/es5b03+25vMyu+VMNC0kWS1khaK+nqAss/Lmm1pEcl3S3ppLxl75H0TPJ4T5p1TjSV5Rmue9syrn3Lmdz/7HYu/uIvePB5d0BoZsWTWlhIygDXA28AlgBXSFoyYrWHgJaIWAr8ALgu2XY68JfAucAK4C8lTUur1onq8hXzuO0DrySTEW//6m/4h7uept+DKJlZEaR5ZLECWBsR6yOiF7gFuDR/hYi4JyKGrhW9D5iTPH89cFdEbI+IHcBdwEUp1jphnTlnKnd85He57KxmvnD3M7z9ht/w3Na9xS7LzEpMmmHRDGzIm25L5o3m/cCPDnPbSa2+qoLPvfMsvnjFctZu2cNFX/g5X/v5ejd+m9kxk2ZYqMC8gt9ukt4FtACfPZRtJV0pqVVSa3t7+2EXery4ZNls7vrYa3jVyU185o4nectXfs2TL3UUuywzKwFphkUbMDdveg5wwM0Dks4H/jdwSUT0HMq2EXFjRLREREtTU9NRK3wiO2FqFV9799l86YrltG3v5OIv/ZK/WvmE+5Yys1SlGRargMWSFkjKApcDK/NXkLQcuIFcUGzJW3QncKGkaUnD9oXJPAMk8aZls7n7E6/hD1fM41u/eY7f//t7+d6qF3xqysxSkVpYREQ/cBW5L/kngVsj4glJ10i6JFnts0Ad8H1JD0tamWy7Hfg0ucBZBVyTzLM8DTVZPn3ZGfzwqldx0oxa/vy2x3jjF37BPWu2EOHQMLOjR5PlS6WlpSVaW1uLXUbRRAQ/enwTf/vjp3h+WyfnLpjOxy44hVcsnFHs0sxsApP0YES0jLmew2Jy6e0f5Dv3P8/1966jfXcPr1w0g4/+/mJWLJiOVOi6ATMrZQ6LEtfdN8C373+Br9y7jq17elg+r4EPvGYRF7xsFmVlDg0zy3FYGJALje8/2MbXfr6eF7Z3sqCxlnf/zkm87ew51FdVFLs8Mysyh4Xtp39gkB89vombfvUsD72wk9pshje/vJkrVszj9NlTi12emRWJw8JG9WjbTr756+e4/dGX6O0f5MzmqbzznLlcvPREGmqyxS7PzI4hh4WNaVdnH//2UBu3rNrAU5t2k82U8brTmnjz8mZee+pMqioyxS7RzFLmsLBxiwie2NjBvz30Iv/x8Ea27umhNpvh/CWzeOOZJ/KaU5ocHGaTlMPCDkv/wCC/Wb+NOx57iR8/vokdnX1UV2R49SmNXLDkBF53ahMz6iqLXaaZHSUOCztifQOD3L9+Oz9ZvYmfPLGZTR3dSLBsTgO/d9pMXn1KE2c2TyXjS3HNjlsOCzuqBgeDxzfu4p6n2rlnzRYeadtJBDTUVHDeyY2ct6iRVy6awUkzanzzn9lxxGFhqdq2p4dfrdvGz59u5xfPtLO5I9dhcHNDNecumM65C6dz7gKHh9lE57CwYyYiWL91L79et43frNvK/eu3s21vLwCNdZWcM38aZ5+Ue5w+eyrZ8lSHfjezQzDesCg/FsXY5CaJRU11LGqq449fcRIRwbr2vdz/7DZan9tB6/Pb+dHjmwDIlpdxxuwpLJ83jWVzG1g+t4E506p99GE2wfnIwo6JzR3d/Pb5HTy0YSe/fX4Hj724i57+QQCm1VRw5pwGzmyewpnNUzl99lQHiNkx4tNQNqH1DQyyZtNuHt6wk8fadvHYi7tYs3n38OBNU6srOH32FJacOIUls6fwshOnsKipzqewzI4yn4ayCa0iU8YZzVM5o3lfv1TdfQOs2bSbxzfu4vEXd7F6Ywf/fN/zw0cgFZnc6a7TTqjn1BOmcNoJ9ZxyQj2zp1b5KMQsZQ4LmzCqKjIsm9vAsrkNw/P6BwZ5duteVr/UwVObdvPUSx3c/+x2/v3hfUOy11WWc/LMOk6ZVcfimfWcPLOOk2fW0dxQ7e7YzY4Sh4VNaOWZMhbPqmfxrHouzZu/q6uPpzfvZs2m3TyzeTdPb97Dfz61hVtb24bXqaooY2FjHYtm1rGwsZaFTbUsaqpjQWMttZX+1Tc7FP4fY8elqdUVnDN/OufMn77f/J2dvazdsodntuxh3ZY9rGvfw8MbdnD7oxvJb56bWV/JgsZaFjTWMr+xlvkzcs/nTa+hOut+sMxGSjUsJF0EfAHIAF+PiGtHLH818HlgKXB5RPwgb9kA8Fgy+UJEXJJmrTY5NNRkaZk/nZYRIdLdN8Dz2zpZ176HZ7fuHX7ctXrz8D0hQ06cWsW86TWcNKOGk2bUDj+fN73GXbhbyUotLCRlgOuBC4A2YJWklRGxOm+1F4D3An9W4CW6IuKstOqz0lJVkeHUE+o59YT6A5Z1dPfx3Na9PLetk+e37uXZbXt5YVsn96xpp313237r1leVM296DXOn1TB3ejVz8543N/ioxCavNI8sVgBrI2I9gKRbgEuB4bCIiOeSZYMp1mF2UFOqKlg6p4GlcxoOWLa3p58NOzp5flsnL2zrZMOOTl7Y3snTW3bzn2u20Nu//6/ujNoszdOqaW7IPWYnj9zzKqbXZn3llh2X0gyLZmBD3nQbcO4hbF8lqRXoB66NiH8/msWZjUdtZTmnnTCF006YcsCywcFg654eNuzoZMP2Ll7c2UXbji7adnSyZvNu7lmzhe6+/cMkW17GiVOrkkc1s6bkns+aUsWsKZWcMLWKprpKyjO+n8QmljTDotCfT4dyB+C8iNgoaSHwn5Iei4h1+72BdCVwJcC8efMOv1Kzw1BWJmZOqWLmlCrOPunA5RHB9r29vLSrmxd3drFxZxebdnWzcVc3L+3sYtVz29nc0U3fwP7/LaRcn1qzplQys76KmfWVzKyvpGlKLkiahqbrKz0olR0zaYZFGzA3b3oOsHGUdQ8QERuTn+sl3QssB9aNWOdG4EbI3cF9hPWaHVWSmFFXyYy6yv1uPsw3OBhs29vL5o5uNnd0s6mjm80dPWxJpjd3dPPYi7vYtqeHwQK/4fWV5TTWV9JYl6WxrpIZwz8raarLMr02mVdbyZTqcp8Cs8OWZlisAhZLWgC8CFwO/OF4NpQ0DeiMiB5JjcB5wHWpVWpWJGVloik5ShgtUCB3c+L2vb1s2d1D++4e2vckP3f3sHVP7vHMlj38Zn0POzv7Cr5GeZmYXptlem2WGXVZptVkmVGbZVrtvp/Ta3I/p9Vkaaip8JGLDUstLCKiX9JVwJ3kLp29KSKekHQN0BoRKyWdA/wbMA14k6S/jojTgZcBNyQN32Xk2ixWj/JWZpNeeaZs+JTXWPqSYNm6p4fte3vZtmff8+17e9mW/HxiYwfb9vTQ0d0/6mvVZDPDwZH/c1pNBVOTn9NqskytqaChuoKGmixTqsrd5jIJuSNBsxLXPzDIjs4+dnT2smNvLzs6c4Gys7OPHXt72d6ZPE+W7+zqY1dXHwf76qivKqehpoKG6ixTqytyj5qKfc8LPKZUV1BfWe4uWo4xdyRoZuNSnikbPhU2XoODQUd3Hzs6+9iZhMmurtzzHcnzXV25gNnV1cfGXV10JPNGNujnKxPUV1Uwpbp8X4hU5R5TayqYUlU+vHxKVS5gpiTT9VUV1GYzbpdJicPCzA5ZWZloqMkmd7TXjnu7iKCzd2A4TIYfnX10dPcNB8qurj46uvvZ1dXHlo49dHTn5o28FHmkTJmoqyynvioXJvVD4VJVPvx8/5+5R11lbrquqpy6rI9uCnFYmNkxI4naynJqK8uZ3VB9yNv39g+yOwmO3d39wyGyu7uf3d19dHQlP7v76Ujmt+3oHF6+p6e/4FVlI9VVluceVeXD4VOb3Tddl+xDXWVmeH8KzavNlpOZJMHjsDCz40a2vGz4cuTDERHs7R1gT/e+UNnTk3u+u7ufvT39SbDknu/p6Wd3snxzRzd7ewbY3d3H3t6B4YG6xlJdkaG2MkNNdihAMtQkoVKT3Tddm81QfcB0htpsOTV5z6uzGSrLy4756TaHhZmVDEnDRwYnTB37yrLRRATdfYPs6dkXKvnP9/YMsLenn729/cnPZLqnf/g03MadXXT1Dgyvc7C2nJHKBDVJcNRkMyyd08CXrlh+2PszHg4LM7NDJInq5K/9Q7kw4GB6+wfp6h1gT28/Xb25UNnbM0BXX+55Z88Anb254OnqHaCzd9+y5sM4pXeoHBZmZhNAtryMbHkZU2sqil1KQb5zxszMxuSwMDOzMTkszMxsTA4LMzMbk8PCzMzG5LAwM7MxOSzMzGxMDgszMxvTpBnPQlI78PwRvEQjsPUolXO8KMV9htLc71LcZyjN/T7UfT4pIprGWmnShMWRktQ6ngFAJpNS3Gcozf0uxX2G0tzvtPbZp6HMzGxMDgszMxuTw2KfG4tdQBGU4j5Dae53Ke4zlOZ+p7LPbrMwM7Mx+cjCzMyX8UvkAAAHmklEQVTGVPJhIekiSWskrZV0dbHrSYukuZLukfSkpCckfTSZP13SXZKeSX5OK3atR5ukjKSHJN2eTC+QdH+yz9+TlC12jUebpAZJP5D0VPKZ/85k/6wlfSz53X5c0nclVU3Gz1rSTZK2SHo8b17Bz1Y5X0y+3x6V9PLDfd+SDgtJGeB64A3AEuAKSUuKW1Vq+oFPRMTLgFcAH0r29Wrg7ohYDNydTE82HwWezJv+W+Afkn3eAby/KFWl6wvAjyPiNGAZuf2ftJ+1pGbgI0BLRJwBZIDLmZyf9TeBi0bMG+2zfQOwOHlcCXzlcN+0pMMCWAGsjYj1EdEL3AJcWuSaUhERL0XEb5Pnu8l9eTST299vJat9C7isOBWmQ9Ic4A+AryfTAn4P+EGyymTc5ynAq4FvAEREb0TsZJJ/1uRG/qyWVA7UAC8xCT/riPg5sH3E7NE+20uBf4qc+4AGSScezvuWelg0AxvyptuSeZOapPnAcuB+YFZEvAS5QAFmFq+yVHwe+J/AYDI9A9gZEf3J9GT8zBcC7cDNyem3r0uqZRJ/1hHxIvB3wAvkQmIX8CCT/7MeMtpne9S+40o9LFRg3qS+PExSHXAb8N8joqPY9aRJ0sXAloh4MH92gVUn22deDrwc+EpELAf2MolOORWSnKO/FFgAzAZqyZ2CGWmyfdZjOWq/76UeFm3A3LzpOcDGItWSOkkV5ILi2xHxr8nszUOHpcnPLcWqLwXnAZdIeo7cKcbfI3ek0ZCcqoDJ+Zm3AW0RcX8y/QNy4TGZP+vzgWcjoj0i+oB/BV7J5P+sh4z22R6177hSD4tVwOLkioksuQaxlUWuKRXJufpvAE9GxOfyFq0E3pM8fw/wH8e6trRExCcjYk5EzCf32f5nRPwRcA/wtmS1SbXPABGxCdgg6dRk1u8Dq5nEnzW500+vkFST/K4P7fOk/qzzjPbZrgTenVwV9Qpg19DpqkNV8jflSXojub82M8BNEfGZIpeUCkmvAn4BPMa+8/f/i1y7xa3APHL/4d4eESMbz457kl4L/FlEXCxpIbkjjenAQ8C7IqKnmPUdbZLOIteonwXWA+8j98fhpP2sJf018E5yV/49BPwpufPzk+qzlvRd4LXkepfdDPwl8O8U+GyT4PwyuaunOoH3RUTrYb1vqYeFmZmNrdRPQ5mZ2Tg4LMzMbEwOCzMzG5PDwszMxuSwMDOzMTksbMKSFJL+Pm/6zyT91VF67W9KetvYax7x+7w96fX1nhHzZ0v6QfL8rOQS7qP1ng2SPljovcwOl8PCJrIe4C2SGotdSL6kt+Lxej/wwYh4Xf7MiNgYEUNhdRZwSGGRd1dyIQ3AcFiMeC+zw+KwsImsn9wQkR8buWDkkYGkPcnP10r6maRbJT0t6VpJfyTpAUmPSVqU9zLnS/pFst7FyfYZSZ+VtCrp//+/5r3uPZK+Q+7GxpH1XJG8/uOS/jaZ9yngVcBXJX12xPrzk3WzwDXAOyU9LOmdkmqTMQtWJR0BXpps815J35f0Q+Ankuok3S3pt8l7D/WYfC2wKHm9zw69V/IaVZJuTtZ/SNLr8l77XyX9WLkxEa7L+/f4ZlLrY5IO+CysNBzsrxOzieB64NGhL69xWga8jFw3zuuBr0fECuUGfPow8N+T9eYDrwEWAfdIOhl4N7kuEc6RVAn8StJPkvVXAGdExLP5byZpNrlxE84mN2bCTyRdFhHXSPo9cneOF7xrNiJ6k1BpiYirktf7G3Jdk/yJpAbgAUk/TTb5HWBpcnduOfDmiOhIjr7uk7SSXKeBZ0TEWcnrzc97yw8l73umpNOSWk9Jlp1FrjfiHmCNpC+R6720ORkjgqQeK0E+srAJLekZ95/IDWwzXquS8Tt6gHXA0Jf9Y+QCYsitETEYEc+QC5XTgAvJ9aXzMLmuUGaQGzgG4IGRQZE4B7g36cSuH/g2ufEkDteFwNVJDfcCVeS6cQC4K6+LDgF/I+lR4KfkuraYNcZrvwr4Z4CIeAp4HhgKi7sjYldEdJPrV+kkcv8uCyV9SdJFwKTuqdhG5yMLOx58HvgtcHPevH6SP3aS/m/yh8vM7/tnMG96kP1/50f2dRPkvoA/HBF35i9I+pbaO0p9hbqBPhIC3hoRa0bUcO6IGv4IaALOjog+5XrXrRrHa48m/99tACiPiB2SlgGvJ3dU8g7gT8a1Fzap+MjCJrzkL+lb2X9IzOfInfaB3DgGFYfx0m+XVJa0YywE1gB3Av9Nue7ckXSKcgMHHcz9wGskNSaN31cAPzuEOnYD9XnTdwIfTkIQSctH2W4qufE6+pK2h5NGeb18PycXMiSnn+aR2++CktNbZRFxG/B/yHV1biXIYWHHi78n18vmkK+R+4J+ABj5F/d4rSH3pf4j4APJ6ZevkzsF89ukUfgGxjgCT7p8/iS57rAfAX4bEYfSFfY9wJKhBm7g0+TC79Gkhk+Pst23gRZJreQC4Kmknm3k2loeH9mwDvwjkJH0GPA94L1j9MLaDNybnBL7ZrKfVoLc66yZmY3JRxZmZjYmh4WZmY3JYWFmZmNyWJiZ2ZgcFmZmNiaHhZmZjclhYWZmY3JYmJnZmP4/GAHBzBhPlsAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import util\n",
    "from itertools import count\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class RNNLanguageModel:\n",
    "    def __init__(self, model, LAYERS, INPUT_DIM, HIDDEN_DIM, VOCAB_SIZE, builder = dy.SimpleRNNBuilder):\n",
    "        \n",
    "        self.builder = builder(LAYERS, INPUT_DIM, HIDDEN_DIM, model)\n",
    "        self.lookup = model.add_lookup_parameters((VOCAB_SIZE, INPUT_DIM), name=\"lookup\")\n",
    "        self.R = model.add_parameters((VOCAB_SIZE, HIDDEN_DIM), name=\"hidden2out\")\n",
    "        self.bias = model.add_parameters((VOCAB_SIZE), name=\"bias\")\n",
    "\n",
    "    def save_to_disk(self, filename):\n",
    "        dy.save(filename, [self.builder, self.lookup, self.R, self.bias])\n",
    "\n",
    "    def load_from_disk(self, filename):\n",
    "        (self.builder, self.lookup, self.R, self.bias) = dy.load(filename, model)\n",
    "        \n",
    "    def build_lm_graph(self, sent):\n",
    "        dy.renew_cg()\n",
    "        init_state = self.builder.initial_state()\n",
    "\n",
    "        errs = [] # will hold expressions\n",
    "        es=[]\n",
    "        state = init_state\n",
    "        for (cw,nw) in zip(sent,sent[1:]):\n",
    "            # assume word is already a word-id\n",
    "            x_t = dy.lookup(self.lookup, int(cw))\n",
    "            state = state.add_input(x_t)\n",
    "            y_t = state.output()\n",
    "            r_t = self.bias + (self.R * y_t)\n",
    "            err = dy.pickneglogsoftmax(r_t, int(nw))\n",
    "            errs.append(err)\n",
    "        nerr = dy.esum(errs)\n",
    "        return nerr\n",
    "    \n",
    "    def predict_next_word(self, sentence):\n",
    "        dy.renew_cg()\n",
    "        init_state = self.builder.initial_state()\n",
    "        state = init_state\n",
    "        for cw in sentence:\n",
    "            # assume word is already a word-id\n",
    "            x_t = self.lookup[int(cw)]\n",
    "            state = state.add_input(x_t)\n",
    "        y_t = state.output()\n",
    "        r_t = self.bias + (self.R * y_t)\n",
    "        prob = dy.softmax(r_t)\n",
    "        return prob\n",
    "        \n",
    "    def sample(self, first=1, nchars=0, stop=-1):\n",
    "        res = [first]\n",
    "        dy.renew_cg()\n",
    "        state = self.builder.initial_state()\n",
    "\n",
    "        cw = first\n",
    "        while True:\n",
    "            x_t = self.lookup[cw]\n",
    "            state = state.add_input(x_t)\n",
    "            y_t = state.output()\n",
    "            r_t = self.bias + (self.R * y_t)\n",
    "            ydist = dy.softmax(r_t)\n",
    "            dist = ydist.vec_value()\n",
    "            rnd = random.random()\n",
    "            for i,p in enumerate(dist):\n",
    "                rnd -= p\n",
    "                if rnd <= 0: break\n",
    "            res.append(i)\n",
    "            cw = i\n",
    "            if cw == stop: break\n",
    "            if nchars and len(res) > nchars: break\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24158\n",
      "<s>żzpVpúsãúIvżf ó\n",
      "2500 24158\n",
      "3.312249622638672\n",
      "<s>Agüaer\n",
      "5000 24158\n",
      "3.3096209765548243\n",
      "<s>Collin\n",
      "7500 24158\n",
      "3.315030470705376\n",
      "<s>Sode\n",
      "10000 24158\n",
      "3.3302472654334774\n",
      "<s>Araikkeskayy\n",
      "12500 24158\n",
      "3.341012694952305\n",
      "<s>Charo\n",
      "15000 24158\n",
      "3.3434368001072747\n",
      "<s>Elusi\n",
      "17500 24158\n",
      "3.341647548855079\n",
      "<s>Cooper\n",
      "20000 24158\n",
      "3.3617703706721214\n",
      "<s>At\n",
      "22500 24158\n",
      "3.3823963349407498\n",
      "<s>Dejchenef\n",
      "TM: 6.840626398722331e-05\n",
      "ITER 0, loss=44670.92741680145\n",
      "0 24158\n",
      "3.3324078639911563\n",
      "<s>Balanov\n",
      "2500 24158\n",
      "3.363216893229123\n",
      "<s>Adniani\n",
      "5000 24158\n",
      "3.371980296670738\n",
      "<s>Sasgol\n",
      "7500 24158\n",
      "3.3632297635702564\n",
      "<s>Vanara\n",
      "10000 24158\n",
      "3.3505029714091425\n",
      "<s>Jilyakov\n",
      "12500 24158\n",
      "3.361541686939542\n",
      "<s>Gurin\n",
      "15000 24158\n",
      "3.3866671085333886\n",
      "<s>Chena\n",
      "17500 24158\n",
      "3.362624272731542\n",
      "<s>Eosion\n",
      "20000 24158\n",
      "3.3636006569254717\n",
      "<s>Mis\n",
      "22500 24158\n",
      "3.3638998851674833\n",
      "<s>Jonsi\n",
      "TM: 6.514787673950195e-05\n",
      "ITER 1, loss=45200.18455505371\n",
      "0 24158\n",
      "3.371889933237875\n",
      "<s>Sapinh\n",
      "2500 24158\n",
      "3.365280220255356\n",
      "<s>Roriechietu\n",
      "5000 24158\n",
      "3.359764774769603\n",
      "<s>Isas\n",
      "7500 24158\n",
      "3.370783594586014\n",
      "<s>Tauseno\n",
      "10000 24158\n",
      "3.382207947423306\n",
      "<s>Juling\n",
      "12500 24158\n",
      "3.341873290827456\n",
      "<s>Arile\n",
      "15000 24158\n",
      "3.371815380475701\n",
      "<s>Manyanbau\n",
      "17500 24158\n",
      "3.3600352293024556\n",
      "<s>Janan\n",
      "20000 24158\n",
      "3.3560370296574376\n",
      "<s>Vofiselfiter\n",
      "22500 24158\n",
      "3.3673505481039108\n",
      "<s>Hva\n",
      "TM: 6.505421229771205e-05\n",
      "ITER 2, loss=44562.630986213684\n",
      "loading the saved model...\n",
      "<s>Elias\n"
     ]
    }
   ],
   "source": [
    "corpus = \"allnames.txt\"\n",
    "\n",
    "LAYERS = 2\n",
    "INPUT_DIM = 32 #50  #256\n",
    "HIDDEN_DIM = 128 # 50  #1024\n",
    "\n",
    "train = util.CharsCorpusReader(corpus, begin=\"<s>\")\n",
    "vocab = util.Vocab.from_corpus(train)\n",
    "\n",
    "VOCAB_SIZE = vocab.size()\n",
    "\n",
    "model = dy.Model()\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.2)\n",
    "\n",
    "lm = RNNLanguageModel(model, LAYERS, INPUT_DIM, HIDDEN_DIM, VOCAB_SIZE, builder=dy.SimpleRNNBuilder)\n",
    "#lm = RNNLanguageModel(model, LAYERS, INPUT_DIM, HIDDEN_DIM, VOCAB_SIZE, builder=dy.LSTMBuilder)\n",
    "\n",
    "\n",
    "train = list(train)\n",
    "\n",
    "losses = []\n",
    "\n",
    "chars = loss = 0.0\n",
    "\n",
    "for ITER in range(3):\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    for i,sent in enumerate(train):\n",
    "        _start = time.time()\n",
    "        \n",
    "        if i % 2500 == 0:\n",
    "            trainer.status()\n",
    "            print (i,len(train))\n",
    "            \n",
    "            if chars > 0: print(loss / chars,)\n",
    "                \n",
    "            for _ in range(1):\n",
    "                samp = lm.sample(first=vocab.w2i[\"<s>\"],stop=vocab.w2i[\"\\n\"])\n",
    "                print(\"\".join([vocab.i2w[c] for c in samp]).strip())\n",
    "            loss = 0.0\n",
    "            chars = 0.0\n",
    "\n",
    "        chars += len(sent)-1\n",
    "        isent = [vocab.w2i[w] for w in sent]\n",
    "        errs = lm.build_lm_graph(isent)\n",
    "        loss += errs.scalar_value()\n",
    "        errs.backward()\n",
    "        trainer.update()\n",
    "    print (\"TM:\",(time.time() - _start)/len(sent))\n",
    "    print(\"ITER {}, loss={}\".format(ITER, loss))\n",
    "    losses.append(loss)\n",
    "    trainer.status()\n",
    "\n",
    "lm.save_to_disk(\"RNNLanguageModel.model\")\n",
    "\n",
    "print(\"loading the saved model...\")\n",
    "lm.load_from_disk(\"RNNLanguageModel.model\")\n",
    "samp = lm.sample(first=vocab.w2i[\"<s>\"],stop=vocab.w2i[\"\\n\"])\n",
    "print(\"\".join([vocab.i2w[c] for c in samp]).strip())\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
